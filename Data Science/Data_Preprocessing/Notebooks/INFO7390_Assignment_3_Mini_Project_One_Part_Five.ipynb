{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Part Five: Encoding </p>\n",
    "![title](https://media.giphy.com/media/H4DjXQXamtTiIuCcRU/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Table of Contents </p>\n",
    "\n",
    "- ## 1. [Introduction](#Introduction)\n",
    "   - ### 1.1 [Abstract](#abstract)\n",
    "- ## 2 [Encoding](#en)\n",
    "   - ### 2.1 [What are Encodings?](#enc)\n",
    "   - ### 2.2 [Reading in files with encoding problems¶](#re)\n",
    "   - ### 2.3 [Saving your files with UTF-8 encoding¶](#sd))\n",
    "   - ### 2.5. [Encoding Techniques- Categorical Data](#m2)\n",
    "      - ####  2.4.1 [Label Encoding](#le)\n",
    "      - ####  2.4.2 [One Hot Encoding](#ohe)\n",
    "      - ####  2.4.3 [Feature Hashing](#fh)\n",
    "      - ####  2.4.4 [Encoding categories with dataset statistics](#ee)\n",
    "      - ####  2.4.5 [Encoding cyclic features ¶](#s)\n",
    "      - ####  2.4.6 [Target Encoding](#tr)\n",
    "      - ####  2.4.7 [K-Fold target encoding](#k)\n",
    "- ## 3. [Conclusion](#Conclusion)\n",
    "- ## 4. [Contribution](#Contribution)\n",
    "- ## 5. [Citation](#Citation)\n",
    "- ## 6. [License](#License)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Introduction </p> <a id='Introduction'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Abstract <a id=\"abstract\">  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this kernel,We are going to learn and try some of the most commonly used encoding techniques. As this competition mainly deals with encoding I hope that it would be a great time to refresh some the most common and effective encoding techniques currently in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.2 Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> 2.0 Encoding </p> <a id='en'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What are encodings? <a id=\"enc\"> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Set\n",
    "Before discussing encodings, we need to introduce a related concept called a character set, which is the set of objects to be encoded. Take plain English, for example. We probably want to encode the letters A–Z (in both upper and lower cases); we then need the numbers 0–9; and probably, we also need the symbols such as “,” and “.”. All these objects together constitute a character set. \n",
    "\n",
    "The standard character set for plain English is the so-called ASCII set. It contains all the objects we just mentioned as well as some other special symbols (dollar and pound signs, some mathematical operators, etc.). Each object in a character set has a unique ID called its code point.\n",
    "\n",
    "However, English is not the only language in the world, and almost any language would need some additional or alternative symbols or objects: Arabic, Chinese, French, German, Hindi, Japanese, Russian, Swedish, etc. For a while, almost every language had its own character set, and the world was accumulating a wealth of character sets and corresponding encodings. In addition to their sheernumber and variety, another problem of having so many distinct character sets and encodings is how very inconvenient such multiplicity made working with text files containing multiple languages. Eventually, Unicode was developed to redress this situation. Unicode has virtually all the characters and symbols in all the languages on earth; it is currently the largest character set in use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Now we turn to encodings. ASCII is also an encoding scheme itself. That is to say, each character in ASCII is mapped to a binary sequence directly by converting its code point (i.e., its numeric ID) to a binary number. For example, the code point for the letter “A” is 65, and it is encoded as 65 in binary—01000001. For Unicode, the scheme had to become more complicated, creating some confusion for many people about encodings. First, Unicode, despite the word code in its root, is not actually an encoding scheme; it is a character set. Then, there are not one but three standard encoding schemes to map a code point in the Unicode set to a binary sequence in the computer. They are UTF-32, UTF-16, and UTF-8, with UTF-8 being the most popular encoding. For example, most webpages on the Internet are encoded by UTF-8. For most users of text analysis, it is not necessary to know the details of the mappings. Nonetheless, we discuss them briefly in the following paragraph for the curious.\n",
    "\n",
    "It might seem straightforward to map a code point in Unicode to a binary sequence directly by converting the code point into binary, just as we do for ASCII. The huge size of Unicode complicates this however. Given how many objects it contains (consider all the Asiatic and Arabic and Cyrillic symbols in addition to the Roman\n",
    "ones in ASCII), the largest code point in Unicode would take 32 bits—32 zeros or ones—in binary. We can certainly encode every code point as a 32-bit binary sequence and that is exactly what UTF-32 does, but a lot of space is wasted. \n",
    "\n",
    "Recall that the code point for “A” is 65, which is 01000001 in ASCII encoding. The code point for “A” is also 65 in Unicode, but if we encode it as a 32-bit binary sequence, then it will be 00000000000000000000000001000001. Although this encoding scheme is simple, it is not economical, as most of the spaces are wasted by padding 0s to get to 32 digits. The variable-length encodings such as UTF-8 and UTF-16 were developed to address this issue. If a code point can be encoded with a single byte (8 bits), then UTF-8 will encode it with a single byte (e.g., the letter “A”); if a single byte is not enough for some code point, UTF-8 will use two bytes (16 bits); if two is not enough, UTF-8 will try three, and so on. \n",
    "\n",
    "UTF-16 is between UTF-8 and UTF-32: it uses two bytes at least and four bytes when necessary. For the variable-length encodings, several bits are used to signal whether the currently encoded object is single-byte or multi-byte, and if multi-byte,the number of bytes in use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Encodings\n",
    "Since there are multiple encodings, a common question comes up when opening a text file: Which encoding is used by this file? (Technically, the process of opening a text file and displaying it on the screen is decoding, but if we know the encoding, then we know how to decode, so we will use the two interchangeably in this\n",
    "guide.) This is an important question because if you open the text file with a wrong encoding scheme, then you might see weird symbols or might fail to open the file altogether. But unfortunately, there is no systematic way to tell what encoding the file uses by merely looking at its filename or content (a pile of 0s and 1s,\n",
    "remember?). If there are no metadata about the encoding for the file, then the best one can do is trial and error. For example, start with ASCII, and if the text does not display correctly, try UTF-8, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# check to see what datatype it is\n",
    "type(before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The other data is the bytes data type, which is a sequence of integers. You can convert a string into bytes by specifying which encoding it's in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"utf-8\", errors = \"replace\")\n",
    "\n",
    "# check the type\n",
    "type(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn't really work for writing any language other than English.) Here you can see that our euro symbol has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an ASCII string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'This is the euro symbol: \\xe2\\x82\\xac'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at what the bytes look like\n",
    "after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the euro symbol: €\n"
     ]
    }
   ],
   "source": [
    "# convert it back to utf-8\n",
    "print(after.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we try to use a different encoding to map our bytes into a string,, we get an error. This is because the encoding we're trying to use doesn't know what to do with the bytes we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\n",
    "\n",
    "You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a cd player. If you try to play a cassette in a CD player, it just won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 25: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-50fd8662e3ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# try to decode our bytes with the ascii encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ascii\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 25: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# try to decode our bytes with the ascii encoding\n",
    "print(after.decode(\"ascii\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n",
    "\n",
    "For example, if we try to convert a string to bytes for ascii using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the euro symbol: ?\n"
     ]
    }
   ],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"ascii\", errors = \"replace\")\n",
    "\n",
    "# convert it back to utf-8\n",
    "print(after.decode(\"ascii\"))\n",
    "\n",
    "# We've lost the original underlying byte string! It's been \n",
    "# replaced with the underlying byte string for the unknown character :(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files, which we'll talk about next.\n",
    "\n",
    "First, however, try converting between bytes and strings with different encodings and see what happens. Notice what this does to your text. Would you want this to happen to data you were trying to analyze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reading in files with encoding problems <a id =\"re\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-71b1e8ee248b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# try to read in a file not in UTF-8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkickstarter_2016\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Datasets\\ks-projects-201612.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte"
     ]
    }
   ],
   "source": [
    "# try to read in a file not in UTF-8\n",
    "kickstarter_2016 = pd.read_csv(\"Datasets\\ks-projects-201612.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn't actually UTF-8. We don't know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "I'm going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) Another reason to just look at the first part of the file is that we can see by looking at the error message that the first problem is the 11th character. So we probably only need to look at the first little bit of the file to figure out what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# helpful character encoding module\n",
    "import chardet\n",
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"Datasets\\ks-projects-201612.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So chardet is 75.25% confidence that the right encoding is \"UTF-8\". Let's see if that's correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rushabh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09 11:36:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26 00:20:50</td>\n",
       "      <td>45000</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16 04:24:11</td>\n",
       "      <td>5000</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29 01:00:00</td>\n",
       "      <td>19500</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283</td>\n",
       "      <td>canceled</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>1283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01 13:38:27</td>\n",
       "      <td>50000</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>52375</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>52375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               name   \\\n",
       "0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1  1000004038                                     Where is Hank?   \n",
       "2  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "3  1000011046  Community Film Project: The Art of Neighborhoo...   \n",
       "4  1000014025                               Monarch Espresso Bar   \n",
       "\n",
       "        category  main_category  currency             deadline   goal   \\\n",
       "0          Poetry     Publishing       GBP  2015-10-09 11:36:00   1000   \n",
       "1  Narrative Film   Film & Video       USD  2013-02-26 00:20:50  45000   \n",
       "2           Music          Music       USD  2012-04-16 04:24:11   5000   \n",
       "3    Film & Video   Film & Video       USD  2015-08-29 01:00:00  19500   \n",
       "4     Restaurants           Food       USD  2016-04-01 13:38:27  50000   \n",
       "\n",
       "             launched  pledged       state  backers  country  usd pledged   \\\n",
       "0  2015-08-11 12:12:28        0      failed        0       GB            0   \n",
       "1  2013-01-12 00:20:50      220      failed        3       US          220   \n",
       "2  2012-03-17 03:24:11        1      failed        1       US            1   \n",
       "3  2015-07-04 08:35:03     1283    canceled       14       US         1283   \n",
       "4  2016-02-26 13:38:27    52375  successful      224       US        52375   \n",
       "\n",
       "  Unnamed: 13 Unnamed: 14 Unnamed: 15  Unnamed: 16  \n",
       "0         NaN         NaN         NaN          NaN  \n",
       "1         NaN         NaN         NaN          NaN  \n",
       "2         NaN         NaN         NaN          NaN  \n",
       "3         NaN         NaN         NaN          NaN  \n",
       "4         NaN         NaN         NaN          NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"Datasets\\ks-projects-201612.csv\", encoding='Windows-1252')\n",
    "\n",
    "# look at the first few lines\n",
    "kickstarter_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, looks like chardet was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be be fine.\n",
    "\n",
    ">What if the encoding chardet guesses isn't right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x96 in position 2: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 2: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-44f65f335036>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# what the correct encoding should be and read in the file. :)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpolice_killings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Datasets\\PoliceKillingsUS.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 2: invalid start byte"
     ]
    }
   ],
   "source": [
    "# what the correct encoding should be and read in the file. :)\n",
    "police_killings = pd.read_csv(\"Datasets\\PoliceKillingsUS.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Saving your files with UTF-8 encoding <a id=\"sd\"> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"Datasets\\ks-projects-201801-utf8.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy, huh? :)\n",
    "\n",
    ">If you haven't saved a file in a kernel before, you need to hit the commit & run button and wait for your >notebook to finish running first before you can see or access the file you've saved out. If you don't see it at >first, wait a couple minutes and it should show up. The files you save will be in the directory \"../output/\", ?>and you can download them from your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Encoding Techniques - Categorical Data <a id=\"#m2\"> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4.1 Method 1: Label encoding <a id='le'> </a>\n",
    "\n",
    "This transformer should be used to encode target values, i.e. y, and not the input X.\n",
    "In this method we change every categorical data to a number.That is each type will be subtuted by a number.for example we will substitute 1 for Grandmaster,2 for master ,3 for expert etc.. For implementing this we will first import Labelencoder from sklearn module.\n",
    "\n",
    "![](https://miro.medium.com/max/2288/1*CK0gfoqvSPBziJcY1TQC6w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_train=pd.read_csv('Datasets/train_cat.csv')\n",
    "#df_test=pd.read_csv('test_cat.csv')\n",
    "X=df_train.drop(['target'],axis=1)\n",
    "y=df_train['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do these three steps to label encode our data:\n",
    "\n",
    "- Initialize the labelencoder class\n",
    "- Call the fit() method to fit the data\n",
    "- Transform data to labelencoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_8</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1686</td>\n",
       "      <td>2175</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>650</td>\n",
       "      <td>11635</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1932</td>\n",
       "      <td>8078</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bin_0  bin_1  bin_2  bin_3  bin_4  nom_0  nom_1  nom_2  nom_3  ...  \\\n",
       "0   0      0      0      0      1      1      1      5      5      3  ...   \n",
       "1   1      0      1      0      1      1      1      4      3      5  ...   \n",
       "2   2      0      0      0      0      1      0      4      4      5  ...   \n",
       "\n",
       "   nom_8  nom_9  ord_0  ord_1  ord_2  ord_3  ord_4  ord_5  day  month  \n",
       "0   1686   2175      2      2      1      7      3    136    2      2  \n",
       "1    650  11635      1      2      3      0      0     93    7      8  \n",
       "2   1932   8078      1      1      4      7     17     31    7      2  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.DataFrame()\n",
    "label=LabelEncoder()\n",
    "for c in  X.columns:\n",
    "    if(X[c].dtype=='object'):\n",
    "        train[c]=label.fit_transform(X[c])\n",
    "    else:\n",
    "        train[c]=X[c]\n",
    "        \n",
    "train.head(3)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the label encoded output train data.We will check the shape of train data now and verify that there is no change in the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data set has got 300000 rows and 24 columns\n"
     ]
    }
   ],
   "source": [
    "print('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4.2 Method 2 : One hot encoding <a id='ohe'> </a>\n",
    "\n",
    "Our second method is encoding each category as a one hot encoding (OHE) vector (or dummy variables). OHE is a representation method that takes each category value and turns it into a binary vector of size |i|(number of values in category i) where all columns are equal to zero besides the category column. Here is a little example:\n",
    "\n",
    "![title](https://miro.medium.com/max/878/1*WXpoiS7HXRC-uwJPYsy1Dg.png)\n",
    "\n",
    "To implement on-hot encoding we will use get_dummies() function in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can use OneHotEncoder() method available in sklearn to convert out data to on-hot encoded data.But this method produces a sparse metrix.The advantage of this methos is that is uses very less memory/cpu resourses. To do that,we need to :\n",
    "\n",
    "- Import OneHotEncoder from sklean.preprocessing\n",
    "- Initialize the OneHotEncoder\n",
    "- Fit and then transform our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data set has got 300000 rows and 316461 columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "one=OneHotEncoder()\n",
    "\n",
    "one.fit(X)\n",
    "train=one.transform(X)\n",
    "\n",
    "print('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_onehotencoding = pd.DataFrame(train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_onehotencoding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4.3 Method 3 : Feature hashing (a.k.a the hashing trick) <a id=\"fh\"> </a>\n",
    "\n",
    "Feature hashing is a very cool technique to represent categories in a “one hot encoding style” as a sparse matrix but with a much lower dimensions. In feature hashing we apply a hashing function to the category and then represent it by its indices. for example, if we choose a dimension of 5 to represent “New York” we will calculate H(New York) mod 5 = 3 (for example) so New York representation will be (0,0,1,0,0).\n",
    "\n",
    "![](https://images.akira.ai/glossary/akira-ai-feature-hashing-technique-ml.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hash=X.copy()\n",
    "for c in X.columns:\n",
    "    X_train_hash[c]=X[c].astype('str')      \n",
    "hashing=FeatureHasher(input_type='string')\n",
    "train=hashing.transform(X_train_hash.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data set has got 300000 rows and 1048576 columns\n"
     ]
    }
   ],
   "source": [
    "print('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_hashing = pd.DataFrame(train.toarray())\n",
    "# df_feature_hashing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 Method 4 :Encoding categories with dataset statistics <a id=\"ee\"> </a>\n",
    "\n",
    "Now we will try to give our models a numeric representation for every category with a small number of columns but with an encoding that will put similar categories close to each other. The easiest way to do it is replace every category with the number of times that we saw it in the dataset. This way if New York and New Jersey are both big cities, they will probably both appear many times in our dataset and the model will know that they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_stat=X.copy()\n",
    "for c in X_train_stat.columns:\n",
    "    if(X_train_stat[c].dtype=='object'):\n",
    "        X_train_stat[c]=X_train_stat[c].astype('category')\n",
    "        counts=X_train_stat[c].value_counts()\n",
    "        counts=counts.sort_index()\n",
    "        counts=counts.fillna(0)\n",
    "        counts += np.random.rand(len(counts))/1000\n",
    "        X_train_stat[c].cat.categories=counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_8</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153535.000053</td>\n",
       "      <td>191633.000043</td>\n",
       "      <td>127341.000729</td>\n",
       "      <td>29855.000193</td>\n",
       "      <td>45979.000698</td>\n",
       "      <td>36942.000557</td>\n",
       "      <td>...</td>\n",
       "      <td>271.000637</td>\n",
       "      <td>19.000490</td>\n",
       "      <td>2</td>\n",
       "      <td>77428.000655</td>\n",
       "      <td>33768.000491</td>\n",
       "      <td>24740.000018</td>\n",
       "      <td>3974.000010</td>\n",
       "      <td>506.000815</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>153535.000053</td>\n",
       "      <td>191633.000043</td>\n",
       "      <td>127341.000729</td>\n",
       "      <td>101181.000572</td>\n",
       "      <td>29487.000452</td>\n",
       "      <td>101123.000408</td>\n",
       "      <td>...</td>\n",
       "      <td>111.000398</td>\n",
       "      <td>13.000644</td>\n",
       "      <td>1</td>\n",
       "      <td>77428.000655</td>\n",
       "      <td>22227.000469</td>\n",
       "      <td>35276.000439</td>\n",
       "      <td>18258.000780</td>\n",
       "      <td>2603.000935</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>146465.000645</td>\n",
       "      <td>191633.000043</td>\n",
       "      <td>96166.000034</td>\n",
       "      <td>101181.000572</td>\n",
       "      <td>101295.000926</td>\n",
       "      <td>101123.000408</td>\n",
       "      <td>...</td>\n",
       "      <td>278.000793</td>\n",
       "      <td>29.000898</td>\n",
       "      <td>1</td>\n",
       "      <td>25065.000705</td>\n",
       "      <td>63908.000023</td>\n",
       "      <td>24740.000018</td>\n",
       "      <td>16927.000603</td>\n",
       "      <td>2572.000845</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bin_0  bin_1  bin_2          bin_3          bin_4          nom_0  \\\n",
       "0   0      0      0      0  153535.000053  191633.000043  127341.000729   \n",
       "1   1      0      1      0  153535.000053  191633.000043  127341.000729   \n",
       "2   2      0      0      0  146465.000645  191633.000043   96166.000034   \n",
       "\n",
       "           nom_1          nom_2          nom_3  ...       nom_8      nom_9  \\\n",
       "0   29855.000193   45979.000698   36942.000557  ...  271.000637  19.000490   \n",
       "1  101181.000572   29487.000452  101123.000408  ...  111.000398  13.000644   \n",
       "2  101181.000572  101295.000926  101123.000408  ...  278.000793  29.000898   \n",
       "\n",
       "  ord_0         ord_1         ord_2         ord_3         ord_4        ord_5  \\\n",
       "0     2  77428.000655  33768.000491  24740.000018   3974.000010   506.000815   \n",
       "1     1  77428.000655  22227.000469  35276.000439  18258.000780  2603.000935   \n",
       "2     1  25065.000705  63908.000023  24740.000018  16927.000603  2572.000845   \n",
       "\n",
       "  day month  \n",
       "0   2     2  \n",
       "1   7     8  \n",
       "2   7     2  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data set has got 300000 rows and 24 columns\n"
     ]
    }
   ],
   "source": [
    "print('train data set has got {} rows and {} columns'.format(X_train_stat.shape[0],X_train_stat.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4.5 Encoding cyclic features <a id='s'> </a>\n",
    "![title](https://miro.medium.com/max/343/1*70cevmU8wNggGJEdLam1lw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our features are cyclic in nature.ie day,month etc.\n",
    "\n",
    "A common method for encoding cyclical data is to transform the data into two dimensions using a sine and consine transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.749279e-01</td>\n",
       "      <td>-0.222521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        day_sin   day_cos\n",
       "0  9.749279e-01 -0.222521\n",
       "1 -2.449294e-16  1.000000\n",
       "2 -2.449294e-16  1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cyclic=X.copy()\n",
    "columns=['day','month']\n",
    "for col in columns:\n",
    "    X_train_cyclic[col+'_sin']=np.sin((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col]))\n",
    "    X_train_cyclic[col+'_cos']=np.cos((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col]))\n",
    "X_train_cyclic=X_train_cyclic.drop(columns,axis=1)\n",
    "\n",
    "X_train_cyclic[['day_sin','day_cos']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4.6 Method 5 : Target encoding <a id=\"tr\"> </a>\n",
    "\n",
    "Target-based encoding is numerization of categorical variables via target. In this method, we replace the categorical variable with just one new numerical variable and replace each category of the categorical variable with its corresponding probability of the target (if categorical) or average of the target (if numerical). The main drawbacks of this method are its dependency to the distribution of the target, and its lower predictability power compare to the binary encoding method.\n",
    "\n",
    "for example,\n",
    "<table style=\"width : 20%\">\n",
    "    <tr>\n",
    "    <th>Country</th>\n",
    "    <th>Target</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>India</td>\n",
    "    <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>China</td>\n",
    "    <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>India</td>\n",
    "    <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>China</td>\n",
    "    <td>1</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>India</td>\n",
    "    <td>1</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Encoding for India = [Number of true targets under the label India/ Total Number of targets under the label India] \n",
    "which is 2/3 = 0.66\n",
    "\n",
    "<table style=\"width : 20%\">\n",
    "    <tr>\n",
    "    <th>Country</th>\n",
    "    <th>Target</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>India</td>\n",
    "    <td>0.66</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>China</td>\n",
    "    <td>0.5</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302537</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.327145</td>\n",
       "      <td>0.360978</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>0.242813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>2</td>\n",
       "      <td>0.403885</td>\n",
       "      <td>0.257877</td>\n",
       "      <td>0.306993</td>\n",
       "      <td>0.208354</td>\n",
       "      <td>0.401186</td>\n",
       "      <td>0.322048</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302537</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.327145</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.359209</td>\n",
       "      <td>0.289954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403885</td>\n",
       "      <td>0.326315</td>\n",
       "      <td>0.206599</td>\n",
       "      <td>0.186877</td>\n",
       "      <td>0.303880</td>\n",
       "      <td>0.340292</td>\n",
       "      <td>0.327496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309384</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.241790</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.293085</td>\n",
       "      <td>0.289954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.317175</td>\n",
       "      <td>0.403126</td>\n",
       "      <td>0.306993</td>\n",
       "      <td>0.351864</td>\n",
       "      <td>0.206843</td>\n",
       "      <td>0.340292</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309384</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.351052</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>0.339793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403885</td>\n",
       "      <td>0.360961</td>\n",
       "      <td>0.330148</td>\n",
       "      <td>0.208354</td>\n",
       "      <td>0.355985</td>\n",
       "      <td>0.322048</td>\n",
       "      <td>0.255729</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n",
       "0   0      0      0      0  0.302537  0.290107  0.327145  0.360978  0.307162   \n",
       "1   1      0      1      0  0.302537  0.290107  0.327145  0.290054  0.359209   \n",
       "2   2      0      0      0  0.309384  0.290107  0.241790  0.290054  0.293085   \n",
       "3   3      0      1      0  0.309384  0.290107  0.351052  0.290054  0.307162   \n",
       "\n",
       "      nom_3  ...     nom_9  ord_0     ord_1     ord_2     ord_3     ord_4  \\\n",
       "0  0.242813  ...  0.368421      2  0.403885  0.257877  0.306993  0.208354   \n",
       "1  0.289954  ...  0.076923      1  0.403885  0.326315  0.206599  0.186877   \n",
       "2  0.289954  ...  0.172414      1  0.317175  0.403126  0.306993  0.351864   \n",
       "3  0.339793  ...  0.227273      1  0.403885  0.360961  0.330148  0.208354   \n",
       "\n",
       "      ord_5       day     month  target  \n",
       "0  0.401186  0.322048  0.244432       0  \n",
       "1  0.303880  0.340292  0.327496       0  \n",
       "2  0.206843  0.340292  0.244432       0  \n",
       "3  0.355985  0.322048  0.255729       1  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_target=df_train.copy()\n",
    "X_target['day']=X_target['day'].astype('object')\n",
    "X_target['month']=X_target['month'].astype('object')\n",
    "for col in X_target.columns:\n",
    "    if (X_target[col].dtype=='object'):\n",
    "        target= dict ( X_target.groupby(col)['target'].agg('sum')/X_target.groupby(col)['target'].agg('count'))\n",
    "        X_target[col]=X_target[col].replace(target).values\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "X_target.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4.7 K-Fold target encoding <a id='k'> </a>\n",
    "\n",
    "k-fold target encoding can be applied to reduce the overfitting. In this method, we divide the dataset into the k-folds, here we consider 5 folds. Fig.3 shows the first round of the 5 fold cross-validation. We calculate mean-target for fold 2, 3, 4 and 5 and we use the calculated values, mean_A = 0.556 and mean_B = 0.285 to estimate mean encoding for the fold-1.\n",
    "\n",
    "![](https://miro.medium.com/max/1955/1*ZKD4eZXzd_FdN0SQDszFVQ.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['target']=y\n",
    "cols=X.drop(['target','id'],axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X_fold=X.copy()\n",
    "X_fold[['ord_0','day','month']]=X_fold[['ord_0','day','month']].astype('object')\n",
    "X_fold[['bin_3','bin_4']]=X_fold[['bin_3','bin_4']].replace({'Y':1,'N':0,'T':1,\"F\":0})\n",
    "kf = KFold(n_splits = 5, shuffle = False, random_state=2019)\n",
    "for train_ind,val_ind in kf.split(X):\n",
    "    for col in cols:\n",
    "        if(X_fold[col].dtype=='object'):\n",
    "            replaced=dict(X.iloc[train_ind][[col,'target']].groupby(col)['target'].mean())\n",
    "            X_fold.loc[val_ind,col]=X_fold.iloc[val_ind][col].replace(replaced).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327356</td>\n",
       "      <td>0.360281</td>\n",
       "      <td>0.305929</td>\n",
       "      <td>0.24171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.334926</td>\n",
       "      <td>0.403542</td>\n",
       "      <td>0.259103</td>\n",
       "      <td>0.307031</td>\n",
       "      <td>0.211418</td>\n",
       "      <td>0.412888</td>\n",
       "      <td>0.323473</td>\n",
       "      <td>0.244538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327356</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.358107</td>\n",
       "      <td>0.289501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.403542</td>\n",
       "      <td>0.327796</td>\n",
       "      <td>0.208194</td>\n",
       "      <td>0.185704</td>\n",
       "      <td>0.293144</td>\n",
       "      <td>0.341711</td>\n",
       "      <td>0.327219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.242135</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.293881</td>\n",
       "      <td>0.289501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.316665</td>\n",
       "      <td>0.402135</td>\n",
       "      <td>0.307031</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>0.208748</td>\n",
       "      <td>0.341711</td>\n",
       "      <td>0.244538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350536</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.305929</td>\n",
       "      <td>0.340791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.403542</td>\n",
       "      <td>0.361036</td>\n",
       "      <td>0.330519</td>\n",
       "      <td>0.211418</td>\n",
       "      <td>0.358066</td>\n",
       "      <td>0.323473</td>\n",
       "      <td>0.255791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350536</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.293881</td>\n",
       "      <td>0.340791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.403542</td>\n",
       "      <td>0.225265</td>\n",
       "      <td>0.208194</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>0.410066</td>\n",
       "      <td>0.341711</td>\n",
       "      <td>0.327219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bin_0  bin_1  bin_2  bin_3  bin_4     nom_0     nom_1     nom_2  \\\n",
       "0   0      0      0      0      1      1  0.327356  0.360281  0.305929   \n",
       "1   1      0      1      0      1      1  0.327356  0.290501  0.358107   \n",
       "2   2      0      0      0      0      1  0.242135  0.290501  0.293881   \n",
       "3   3      0      1      0      0      1  0.350536  0.290501  0.305929   \n",
       "4   4      0      0      0      0      0  0.350536  0.290501  0.293881   \n",
       "\n",
       "      nom_3  ...     nom_9     ord_0     ord_1     ord_2     ord_3     ord_4  \\\n",
       "0   0.24171  ...       0.5  0.334926  0.403542  0.259103  0.307031  0.211418   \n",
       "1  0.289501  ...     0.125  0.278366  0.403542  0.327796  0.208194  0.185704   \n",
       "2  0.289501  ...  0.166667  0.278366  0.316665  0.402135  0.307031  0.354919   \n",
       "3  0.340791  ...  0.233333  0.278366  0.403542  0.361036  0.330519  0.211418   \n",
       "4  0.340791  ...  0.137931  0.278366  0.403542  0.225265  0.208194  0.354919   \n",
       "\n",
       "      ord_5       day     month target  \n",
       "0  0.412888  0.323473  0.244538      0  \n",
       "1  0.293144  0.341711  0.327219      0  \n",
       "2  0.208748  0.341711  0.244538      0  \n",
       "3  0.358066  0.323473  0.255791      1  \n",
       "4  0.410066  0.341711  0.327219      0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <p style=\"text-align: center;\"> 3. Conclusion </p><a id=\"Conclusion\"> </a>\n",
    "\n",
    "This Notebook Contains UTF-8 to ASCII conversions and various categorical data conversion techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <p style=\"text-align: center;\"> 4. Contribution </p> <a id='Contribution'> </a>\n",
    "\n",
    "This was a fun project in which we explore the idea of Data cleaning and Data Preprocessing. We take inspiration from kaggle learning course and create our own notebook enhancing the same idea and supplementing it with our own contributions from our experiences and past projects.\n",
    "       \n",
    "- Code by self : 65%\n",
    "- Code from external Sources : 35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <p style=\"text-align: center;\"> 5. Citations </p> <a id=\"Citation\"> </a>\n",
    "\n",
    "- https://methods.sagepub.com/base/download/DatasetStudentGuide/encodings-in-isis-twitter-2016-python\n",
    "- https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/\n",
    "- https://www.kaggle.com/shahules/an-overview-of-encoding-techniques\n",
    "- https://www.kaggle.com/shivan118/end-to-end-regular-expressions-tutorial\n",
    "- https://medium.com/jbennetcodes/dealing-with-datetimes-like-a-pro-in-pandas-b80d3d808a7f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    <p style=\"text-align: center;\"> 6. License </p> <a id=\"License\"> </a>\n",
    "Copyright (c) 2020 Manali Sharma, Rushabh Nisher\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
