{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Mini Project One: Data Preprocessing</p>\n",
    "![title](Notebooks\\Images\\Data_Wrangling.bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> 1.0 Abstract </p> <a id='abstract'></a>\n",
    "\n",
    "We live in a world where the data is getting bigger by the second. The value of the data can diminish over time if not used properly. Finding anomalies in a dataset is crucial to identifying problems in the business or building a proactive solution to potentially discover the problem before it happens or even in the exploratory data analysis (EDA) phase to prepare a dataset for ML.\n",
    "\n",
    "In a real world data, is not improbable that data may contain incomplete, inconsistent or missing values and maybe in an unusable format which cannot be directly used for machine learning models. If the data is corrupted then it may hinder the process or provide inaccurate results. Data preprocessing is required tasks for cleaning the data and making it suitable for a machine learning model which also increases the accuracy and efficiency of a machine learning model.\n",
    "\n",
    "In this project we will go through the Kaggle Data Science Course on Data Cleaning and Create our own notebook with hands-on example on why is it important to clean data, and what erronous effect it might have on our predictions if we don't perform data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS-** We have done this project in multiple parts, and all parts are divided into seperate notebooks. And these individual notebooks contain the whole code and documentation of the entire part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Index </p>\n",
    "- # 1 [Abstract](#abstract)\n",
    "- # 2 [Part One: Handling Missing Values](./Notebooks/INFO7390_Assignment_3_Mini_Project_One_Part_One.ipynb)\n",
    "- # 3 [Part Two: Standardization and Normalization](./Notebooks/INFO7390_Assignment_3_Mini_Project_One_Part_Two.ipynb)\n",
    "- # 4 [Part Three: Outliers](./Notebooks/INFO7390_Assignment_3_Mini_Project_One_Part_Three.ipynb)\n",
    "- # 5 [Part Four: Dates](./Notebooks/INFO7390_Assignment_3_Mini_Project_One_Part_Four.ipynb)\n",
    "- # 6 [Part Five: Encoding](./Notebooks/INFO7390_Assignment_3_Mini_Project_One_Part_Five.ipynb)\n",
    "- # 7 [Part Six: Fuzzy Matching](./Notebooks/INFO7390_Assignment_3_Mini_Project_One_Part_Six.ipynb)\n",
    "- # 8 [Part Seven: Regular Expressions](./Notebooks/INFO7390_Assignment_3_Mini_Project_One_Part_Seven.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Part One: Handling Missing Values:-**\n",
    "One of the most common problems we have faced in Data Cleaning/Exploratory Analysis is handling the missing values. Firstly, understand that there is \"NO good way to deal with missing data\". We have come across different solutions for data imputation depending on the kind of problem — Time series Analysis, ML, Regression etc. and it is difficult to provide a general solution. In this notebook,we are attempting to summarize the most commonly used methods and trying to find a structural solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Part Two: Standardization and Normalization:-**\n",
    "Machine learning models learn a mapping from input variables to an output variable.\n",
    "As such, the scale and distribution of the data drawn from the domain may be different for each variable.\n",
    "Input variables may have different units (e.g. feet, kilometers, and hours) that, in turn, may mean the variables have different scales.\n",
    "Differences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error.\n",
    "In this notebook we will be tackling this problem and providing general solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Part Three: Outliers:-**\n",
    "Contrary to what most data science courses would have you believe, not every dataset is a perfectly curated group of observations with no missing values or outliers (For example mtcars and iris datasets). Real-world data is messy which means we need to clean and wrangle it into an acceptable format before we can even start the analysis. Data cleaning is an un-glamorous, but necessary part of most actual data science problems. In this notebook, I will try to explain what are outliers and it's types, how to detect outliers and also remidial measures for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Part Four: Dates:-** \n",
    "Working with dates and times is one of the biggest challenges in programming. Between dealing with time zones, daylight saving time, and different written date formats, it can be tough to keep track of which days and times you’re referencing. Fortunately, the built-in Python datetime module can help you manage the complex nature of dates and times. A date in Python is not a data type of its own, but we can import a module named datetime to work with dates as date objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Part Five: Encoding:-** \n",
    "Through this kernel, we are going to learn and try some of the most commonly used encoding techniques.As this competition mainly deals with encoding I hope that it would be a great time to refresh some the most common and effective encoding techniques currently in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Part Six: Fuzzy Matching:-** \n",
    "As a data scientist, you are forced to retrieve information from various sources by either leveraging publicly available API’s, asking for data, or by simply scraping your own data from a web page. All this information is useful if we are able to combine it and not have any duplicates in the data. But how do we make sure that there are no duplicates?\n",
    "I know … “duh! you can just use a function that retrieves all the unique information thus removing duplicates”. Well, that’s one way, but our function probably can’t tell that a name like “Barack Obama” is the same as “Barack H. Obama” right? (Assuming we were retrieving names of the most famous people in the world). We can clearly tell that these names are different but they are probably referring to the same person. So, how do we match these names?\n",
    "This is where Fuzzy String Matching comes in. This post will explain what Fuzzy String Matching is together with its use cases and give examples using Python’s Library Fuzzywuzzy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Part Seven: Regex:-** \n",
    "A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings, using a specialized syntax held in a pattern. Regular expressions are widely used in UNIX world. The Python module re provides full support for Perl-like regular expressions in Python. The re module raises the exception re.error if an error occurs while compiling or using a regular expression. We would cover two important functions, which would be used to handle regular expressions. But a small thing first: There are various characters, which would have special meaning when they are used in regular expression. To avoid any confusion while dealing with regular expressions, we would use Raw Strings as r'expression'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Contribution<p><a id='Contribution'></a>\n",
    "\n",
    "    \n",
    "- Code by self : 65%\n",
    "- Code from external Sources : 35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">  License<p><a id='License'></a>\n",
    "Copyright (c) 2020 Rushabh Nisher, Manali Sharma\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
