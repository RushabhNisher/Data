{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Mini Project Two: Reinforcement Learning</p>\n",
    "![title](Images\\RL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Table of Contents </p>\n",
    "- ## 1. [Introduction](#Introduction)\n",
    "   - ### 1.1 [Abstract](#abstract)\n",
    "\n",
    "- ## 2. [What is Reinforcement Learning](#RL)   \n",
    "   - ### 2.1 [Markov Decision Process](#mdp)\n",
    "   - ### 2.2 [Exploration and Exploitation](#exploration_exploitation)\n",
    "   - ### 2.3 [Policies & Value Functions](#Policies_&_ValueFunctions)\n",
    "   \n",
    "\n",
    "- ## 3. [Implementation](#Implementation)   \n",
    "   - ### 3.1 [Importing Libraries](#importing_libraries)\n",
    "   - ### 3.2 [Defining Agents](#Defining_Agents)\n",
    "   - ### 3.3 [One-Step Lookahead](##one_step)\n",
    "   - ### 3.4 [N-Step Lookahead](##n_step)\n",
    "   - ### 3.5 [Beat the Bot](##beat_the_bot)\n",
    "   \n",
    "- ## 5. [Conclusion](#Conclusion)\n",
    "- ## 6. [Contribution](#Contribution)\n",
    "- ## 7. [Citation](#Citation)\n",
    "- ## 8. [License](#License)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> 1.0 Introduction </p> <a id='Introduction'></a>\n",
    "\n",
    "\n",
    "Welcome to Reinforcement Learning. Reinforcement Learning is a rapidly growing subset of Machine Learning which involves agents attempting to take actions or make moves in their environment in hopes of maximizing some prioritized reward. So how does Reinforcement Learning compare to other Machine Learning methodologies?\n",
    "\n",
    "![title](Images\\ML_Types.png)\n",
    "\n",
    "Let’s see a comparison between RL and others:\n",
    "\n",
    "- **Supervised vs Reinforcement Learning:** In supervised learning, there’s an external “supervisor”, which has knowledge of the environment and who shares it with the agent to complete the task. But there are some problems in which there are so many combinations of subtasks that the agent can perform to achieve the objective. So that creating a “supervisor” is almost impractical. For example, in a chess game, there are tens of thousands of moves that can be played. So creating a knowledge base that can be played is a tedious task. In these problems, it is more feasible to learn from one’s own experiences and gain knowledge from them. This is the main difference that can be said of reinforcement learning and supervised learning. In both supervised and reinforcement learning, there is a mapping between input and output. But in reinforcement learning, there is a reward function which acts as a feedback to the agent as opposed to supervised learning.\n",
    "\n",
    "- **Unsupervised vs Reinforcement Leanring:** In reinforcement learning, there’s a mapping from input to output which is not present in unsupervised learning. In unsupervised learning, the main task is to find the underlying patterns rather than the mapping. For example, if the task is to suggest a news article to a user, an unsupervised learning algorithm will look at similar articles which the person has previously read and suggest anyone from them. Whereas a reinforcement learning algorithm will get constant feedback from the user by suggesting few news articles and then build a “knowledge graph” of which articles will the person like.\n",
    "\n",
    ">**PS:** There is also a fourth type of machine learning methodology called semi-supervised learning, which is essentially a combination of supervised and unsupervised learning. It differs from reinforcement learning as similar to supervised and semi-supervised learning has direct mapping whereas reinforcement does not.\n",
    "\n",
    "\n",
    "#   1.1 Abstract  <a id='abstract'></a>\n",
    "\n",
    "This notebook will serve as a beginners guide to Reinforcement Learning with application on 2048 game. Here we will explore Reinforcement Learning – a goal-oriented learning based on interaction with environment for a newbie to Reinforcement Learning and create a deep q-learning model on 2048 game. This notebook will contain concepts like Value, Action, Policy, Q-Value etc and hands on implementation with examples.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> 2.0 What is Reinforcement Learning </p> <a id='RL'></a>\n",
    "\n",
    "Reinforcement Learning is a major and upcoming field in AI today. Reinforcement Learning is the term used to denote the set of algorithms that have the potential capability to make highly-intelligent decisions depending on their local environment.\n",
    "Reinforcement Learning (RL) is the problem of studying an agent in an environment, the agent has to interact with the environment in order to maximize some cumulative rewards. In other words, it is an iterative feedback loop between an agent and its environment. \n",
    "\n",
    "Example of RL is an agent in a labyrinth trying to find its way out. The fastest it can find the exit, the better reward it will get.\n",
    "\n",
    "![title](Images\\RL_Labrynth.png)\n",
    "\n",
    "There are 5 elements of a basic RL algorithm: \n",
    "- **Agent:** Which can choose to commit to actions in its current state\n",
    "- **Environment:** Responds to action and provides new input to agent\n",
    "- **State:** A set of different states that the environment can exist in\n",
    "- **Action:** The action that the agent takes in order to maximize the reward which changes the state of the environment\n",
    "- **Reward:** Incentive or cumulative mechanism returned by environment\n",
    "\n",
    "\n",
    "The basic schema for a RL algorithm is given below:\n",
    "![title](Images\\RL_Flowchart.jpeg)\n",
    "\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   2.1 Markov Decision Process  <a id='mdp'></a>\n",
    "\n",
    "To describe this problem in a mathematical way, we use Markov Decision Process (MDP).\n",
    "MDP describes the environment as follows.\n",
    "\n",
    "MDP is a collection of States, Actions, Transition Probabilities, Rewards, Discount Factor: (S, A, P, R, γ)\n",
    "\n",
    "- **S:** A set of a finite state that describes the environment.\n",
    "- **A:** A set of a finite actions that describes the action that can be taken by the agent.\n",
    "- **P:** A probability matrix that tells the probability of moving from one state to the other.\n",
    "- **R:** A set of rewards that depend on the state and the action taken. Rewards are not necessarily positive, they should be seen as outcome of an action done by the agent when it is at a certain state. So negative reward indicates bad result, whereas positive reward indicates good result.\n",
    "- **γ:** A discount factor, that tells how important future rewards are to the current state. Discount factor is a value between 0 and 1. A reward R that occurs N steps in the future from the current state, is multiplied by γ^N to describe its importance to the current state. For example consider γ = 0.9 and a reward R = 10 that is 3 steps ahead of our current state. The importance of this reward to us from where we stand is equal to (0.9³)*10 = 7.29.\n",
    "- **G:** The expected long-term return with discount, as opposed to the short-term reward R. Vπ(s) is defined as the expected long-term return of the current state sunder policy π.\n",
    "\n",
    "\n",
    "We have to take an action (A) to transition from our start state to our end state (S). In return getting rewards (R) for each action we take. Our actions can lead to a positive reward or negative reward.\n",
    "\n",
    "The set of actions we took define our policy (π) and the rewards we get in return defines our value (V). Our task here is to maximize our rewards by choosing the correct policy. So we have to maximize \n",
    "\n",
    "\\begin{equation*}\n",
    "E(r_{t}|\\pi s_{t})       \n",
    "\\end{equation*}\n",
    "\n",
    "for all possible values of S for a time t.\n",
    "\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   2.2 Exploration and Exploitation  <a id='exploration_exploitation'></a>\n",
    "\n",
    "\n",
    "The broad goal of most RL algorithms is to achieve a balance between exploration (training on new data points) and exploitation (use of previously captured data). The immediate goal is to maximize the reward with trials alternating between the aforementioned exploitation and exploration.\n",
    "\n",
    "Suppose you have many slot machines with random payouts. A slot machine would look something like this.\n",
    "\n",
    "![title](Images\\Slot_Machine.gif)\n",
    "\n",
    "Now you want to do is get the maximum bonus from the slot machines as fast as possible. What would you do?\n",
    "\n",
    ">One naive approach might be to select only one slot machine and keep pulling the lever all day long. Sounds boring, but it may give you “some” payouts. With this approach, you might hit the jackpot (with a probability close to 0.00000….1) but most of the time you may just be sitting in front of the slot machine losing money. Formally, this can be defined as a pure **exploitation approach**. Is this the optimal choice? The answer is NO.\n",
    "\n",
    ">Let’s look at another approach. We could pull a lever of each & every slot machine and pray to God that at least one of them would hit the jackpot. This is another naive approach which would keep you pulling levers all day long, but give you sub-optimal payouts. Formally this approach is a pure **exploration approach**.\n",
    "\n",
    "Both of these approaches are not optimal, and we have to find a proper balance between them to get maximum reward. This is said to be exploration vs exploitation dilemma of reinforcement learning.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   2.3 Policies  & Value Functions <a id='Policies_&_ValueFunctions'></a>\n",
    "\n",
    "It is important to note that are three types of RL implementations: policy-based, value-based, and model-based. Policy-based RL involves coming up with a policy or deterministic/stochastic strategy to maximize the cumulative reward. Value-based RL attempts to maximize an arbitrary value function, V(s). Model-based RL is based on creating a virtual model for a certain environment and the agent learns to perform within the constraints of the environment.\n",
    "\n",
    "A policy is a function that maps a given state to probabilities of selecting each possible action from that state. We will use the symbol π to denote a policy.\n",
    "\n",
    "When speaking about policies, formally we say that an agent “follows a policy.” For example, if an agent follows policy π\n",
    "at time *t*, then π(a|s) is the probability that $\\pmb{A_{t} = a}$   if $\\pmb{S_{t} = s}$ This means that, at time *t*, under policy π, the probability of taking action *a* in state *s* is $\\pmb{π(a|s)}$.\n",
    "\n",
    "**Note:** For each state s ∈ **S**, π is a probability distribution over a ∈ **A(s)**\n",
    "\n",
    "## Value Functions\n",
    "\n",
    "Value functions are functions of states, or of state-action pairs, that estimate how good it is for an agent to be in a given state, or how good it is for the agent to perform a given action in a given state.\n",
    "\n",
    "This notion of how good a state or state-action pair is is given in terms of expected return. Remember, the rewards an agent expects to receive are dependent on what actions the agent takes in given states. So, value functions are defined with respect to specific ways of acting. Since the way an agent acts is influenced by the policy it's following, then we can see that value functions are defined with respect to policies.\n",
    "\n",
    "### State-Value Function\n",
    "\n",
    "The state-value function for policy **π**, denoted as $\\pmb{v_{π}}$, tells us how good any given state is for an agent following policy **π**. In other words, it gives us the value of a state under **π**.\n",
    "\n",
    "Formally, the value of state **s** under policy **π** is the expected return from starting from state **s** at time **t** and following policy **π** thereafter. Mathematically we define $\\pmb{v_{π}(s)}$ as\n",
    "\n",
    " ![title](Images\\State-Value_Function.png)\n",
    " \n",
    " ### Action-Value Function\n",
    " \n",
    "Similarly, the action-value function for policy **π**, denoted as $\\pmb{q_{π}}$, tells us how good it is for the agent to take any given action from a given state while following policy **π**. In other words, it gives us the value of an action under **π**.\n",
    "\n",
    "Formally, the value of action $\\pmb{a}$ in state $\\pmb{s}$ under policy **π** is the expected return from starting from state $\\pmb{s}$ at time $\\pmb{t}$, taking action $\\pmb{a}$, and following policy **π** thereafter. Mathematically, we define $\\pmb{q_{π}(s,a)}$ as\n",
    "\n",
    "![title](Images\\Action-Value_Function.png)\n",
    "\n",
    "Conventionally, the action-value function $\\pmb{q_{π}}$ is referred to as the Q-function, and the output from the function for any given state-action pair is called a Q-value. The letter “ Q” is used to represent the quality of taking a given action in a given state. We’ll be working with Q-value functions a lot going forward.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> 3.0 Implementation of Reinforcement Learning </p> <a id='Implementation'></a>\n",
    "\n",
    "# Connect 4\n",
    "\n",
    "Connect Four is a game where two players alternate turns dropping colored discs into a vertical grid. Each player uses a different color (usually red or yellow), and the objective of the game is to be the first player to get four discs in a row.\n",
    "\n",
    "![title](Images\\Connect4.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   3.1 Importing Libraries  <a id='importing_libraries'></a>\n",
    "\n",
    "This is the official start to any Data Science or Machine Learning Project. A Python library is a reusable chunk of code that you may want to include in your programs/ projects. \n",
    "In this step we import a few libraries that are required in our program. Some major libraries that are used are Numpy, Pandas, MatplotLib, Seaborn, Sklearn etc.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from kaggle_environments import make, evaluate, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from random import choice\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "# from astropy.table import Table, Column\n",
    "\n",
    "seed = 2176\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the game environment\n",
    "# Set debug=True to see the errors if your agent refuses to run\n",
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "# List of available default agents\n",
    "print(list(env.agents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"random\" agent selects (uniformly) at random from the set of valid moves. In Connect Four, a move is considered valid if there's still space in the column to place a disc (i.e., if the board has seven rows, the column has fewer than seven discs).\n",
    "\n",
    "In the code cell below, this agent plays one game round against a copy of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([\"random\", \"random\"])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Defining agents <a id='Defining_Agents'></a>\n",
    "\n",
    "To participate in the competition, you'll create your own agents.\n",
    "\n",
    "Your agent should be implemented as a Python function that accepts two arguments: obs and config. It returns an integer with the selected column, where indexing starts at zero. So, the returned value is one of 0-6, inclusive.\n",
    "\n",
    "We'll start with a few examples, to provide some context. In the code cell below:\n",
    "\n",
    "- The first agent behaves identically to the \"random\" agent above.\n",
    "\n",
    "- The second agent always selects the middle column, whether it's valid or not! Note that if any agent selects an invalid move, it loses the game.\n",
    "\n",
    "- The third agent selects the leftmost valid column.\n",
    "\n",
    "So, what are `obs` and `config`, exactly?\n",
    "\n",
    "**obs**\n",
    "obs contains two pieces of information:\n",
    "\n",
    "- `obs.board` - the game board (a Python list with one item for each grid location)\n",
    "- `obs.mark` - the piece assigned to the agent (either 1 or 2)\n",
    "\n",
    "`obs.board` is a Python list that shows the locations of the discs, where the first row appears first, followed by the second row, and so on. We use 1 to track player 1's discs, and 2 to track player 2's discs. For instance, for this game board:\n",
    "\n",
    "![title](Images\\Connect4_Board.png)\n",
    "\n",
    "`obs.board` would be \n",
    "\n",
    "`[0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 1, 1, 0, 0, 0, \n",
    "  0, 0, 2, 2, 0, 0, 0, \n",
    "  0, 2, 1, 2, 0, 0, 0, \n",
    "  0, 1, 1, 1, 0, 0, 0, \n",
    "  0, 2, 1, 2, 0, 2, 0]`\n",
    "\n",
    "**config**\n",
    "`config` contains three pieces of information:\n",
    "\n",
    "- `config.columns` - number of columns in the game board (7 for Connect Four)\n",
    "- `config.rows` - number of rows in the game board (6 for Connect Four)\n",
    "- `config.inarow` - number of pieces a player needs to get in a row in order to win (4 for Connect Four)\n",
    "\n",
    "\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects random valid column\n",
    "def agent_random(obs, config):\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    return random.choice(valid_moves)\n",
    "\n",
    "# Selects middle column\n",
    "def agent_middle(obs, config):\n",
    "    return config.columns//2\n",
    "\n",
    "# Selects leftmost valid column\n",
    "def agent_leftmost(obs, config):\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    return valid_moves[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating agents\n",
    "To have the custom agents play one game round, we use the same env.run() method as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents play one game round\n",
    "env.run([agent_leftmost, agent_random])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the result here, agent_leftmost will have a competitive edge over the random agent almost all the time. This is because random agent will not try to interrupt the agent_leftmost's strategy of filling the leftmost available space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents play one game round\n",
    "env.run([agent_middle, agent_random])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent_middle, while being better than agent random, is certainly lackluster, and still does not perform nearly as well, as many number of times as we would like it to. We can try this out by playing the above game a few times and seeing that there are instances where random wins, or middle loses because it tries to make an illegal move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents play one game round\n",
    "env.run([agent_middle, agent_leftmost])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This game is generally won by the player which gets the first chance to play as their respective policies are filling the leftmost and the middle parts of the boards respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome of a single game is usually not enough information to figure out how well our agents are likely to perform. To get a better idea, we'll calculate the win percentages for each agent, averaged over multiple games. For fairness, each agent goes first half of the time.\n",
    "\n",
    "To do this, we'll use the get_win_percentages() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate which agent performs better amongst these 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent_middle, agent2=agent_random, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent_leftmost, agent2=agent_random, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent_leftmost, agent2=agent_middle, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like agent_leftmost outperforms agent_middle, when they are both playing against a random move choosing agent_random. But when we try to compare the middle and the left agent, they seem to be performing equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try a bit more advance technique in this game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 One-Step Lookahead  <a id='one_step'></a>\n",
    "\n",
    "When we play this game as a human player, we don't choose a random strategy like leftmost or middle or random, we weigh our moves and think about alternatives. In essence, we do a bit of forecasting.\n",
    "\n",
    "For each potential move, we predict what out opponent is likely to do in response, and what we would do in response to that and so on. After weighing these options we select the move which will most likely result in our victory. In terms of machine or algorithms, we can formalize this idea and represent all our outcomes in a game tree.\n",
    "\n",
    "![title](Images\\Connect4_Tree.png)\n",
    "\n",
    "The game tree represents all possible moves by both our agent and the opponent and states of the board. It starts with an empty board and iteratively fills out all the rows starting with an empty board. The first row shows all possible moves the agent (red player) can make. Next, we record each move the opponent (yellow player) can make in response, and so on, until each branch reaches the end of the game. (The game tree for Connect Four is quite large, so we show only a small preview in the image above.) Once we can see every way the game can possibly end, it can help us to pick the move where we are most likely to win.\n",
    "\n",
    "## Heuristics\n",
    "\n",
    "The complete game tree for Connect Four has over 4 trillion different boards! So in practice, our agent only works with a small subset when planning a move.\n",
    "\n",
    "To make sure the incomplete tree is still useful to the agent, we will use a heuristic (or heuristic function). The heuristic assigns scores to different game boards, where we estimate that boards with higher scores are more likely to result in the agent winning the game. \n",
    "\n",
    "For instance, one heuristic that might work reasonably well for Connect Four looks at each group of four adjacent locations in a (horizontal, vertical, or diagonal) line and assigns:\n",
    "\n",
    "- 1000000 (1e6) points if the agent has four discs in a row (the agent won),\n",
    "- 1 point if the agent filled three spots, and the remaining spot is empty (the agent wins if it fills in the empty spot), and\n",
    "- -100 points if the opponent filled three spots, and the remaining spot is empty (the opponent wins by filling in the empty spot).\n",
    "\n",
    "This is also represented in the image below.\n",
    "\n",
    "![title](Images\\Heuristic.png)\n",
    "\n",
    "And how exactly will the agent use the heuristic? Consider it's the agent's turn, and it's trying to plan a move for the game board shown at the top of the figure below. There are seven possible moves (one for each column). For each move, we record the resulting game board.\n",
    "\n",
    "![title](Images\\Connect4_Heuristic.png)\n",
    "\n",
    "Then we use the heuristic to assign a score to each board. To do this, we search the grid and look for all occurrences of the pattern in the heuristic, similar to a word search puzzle. Each occurrence modifies the score. For instance,\n",
    "\n",
    "- The first board (where the agent plays in column 0) gets a score of 2. This is because the board contains two distinct patterns that each add one point to the score (where both are circled in the image above).\n",
    "- The second board is assigned a score of 1.\n",
    "- The third board (where the agent plays in column 2) gets a score of 0. This is because none of the patterns from the heuristic appear in the board.\n",
    "\n",
    "The first board receives the highest score, and so the agent will select this move. It's also the best outcome for the agent, since it has a guaranteed win in just one more move. Check this in figure now, to make sure it makes sense to you!\n",
    "\n",
    "The heuristic works really well for this specific example, since it matches the best move with the highest score. It is just one of many heuristics that works reasonably well for creating a Connect Four agent, and you may find that you can design a heuristic that works much better!\n",
    "\n",
    "In general, if you're not sure how to design your heuristic (i.e., how to score different game states, or which scores to assign to different conditions), often the best thing to do is to simply take an initial guess and then play against your agent. This will let you identify specific cases when your agent makes bad moves, which you can then fix by modifying the heuristic.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our one-step lookahead agent will:\n",
    "\n",
    "- use the heuristic to assign a score to each possible valid move, and\n",
    "- select the move that gets the highest score. (If multiple moves get the high score, we select one at random.)\n",
    "\n",
    "\"One-step lookahead\" refers to the fact that the agent looks only one step (or move) into the future, instead of deeper in the game tree.\n",
    "\n",
    "To define this agent, we will use the functions in the code cell below. These functions will make more sense when we use them to specify the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates score if agent drops piece in selected column\n",
    "def score_move(grid, col, mark, config):\n",
    "    next_grid = drop_piece(grid, col, mark, config)\n",
    "    score = get_heuristic(next_grid, mark, config)\n",
    "    return score\n",
    "\n",
    "# Helper function for score_move: gets board at next step if agent drops piece in selected column\n",
    "def drop_piece(grid, col, mark, config):\n",
    "    next_grid = grid.copy()\n",
    "    for row in range(config.rows-1, -1, -1):\n",
    "        if next_grid[row][col] == 0:\n",
    "            break\n",
    "    next_grid[row][col] = mark\n",
    "    return next_grid\n",
    "\n",
    "# Helper function for score_move: calculates value of heuristic for grid\n",
    "def get_heuristic(grid, mark, config):\n",
    "    num_threes = count_windows(grid, 3, mark, config)\n",
    "    num_fours = count_windows(grid, 4, mark, config)\n",
    "    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "    score = num_threes - 1e2*num_threes_opp + 1e6*num_fours\n",
    "    return score\n",
    "\n",
    "# Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
    "def check_window(window, num_discs, piece, config):\n",
    "    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
    "    \n",
    "# Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
    "def count_windows(grid, num_discs, piece, config):\n",
    "    num_windows = 0\n",
    "    # horizontal\n",
    "    for row in range(config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[row, col:col+config.inarow])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # vertical\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns):\n",
    "            window = list(grid[row:row+config.inarow, col])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # positive diagonal\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # negative diagonal\n",
    "    for row in range(config.inarow-1, config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    return num_windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-step lookahead agent is defined in the next code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent is always implemented as a Python function that accepts two arguments: obs and config\n",
    "def agent(obs, config):\n",
    "    # Get list of valid moves\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Convert the board to a 2D grid\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Use the heuristic to assign a score to each possible board in the next turn\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Select at random from the maximizing columns\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the game board to a 2D numpy array. For Connect Four, grid is an array with 6 rows and 7 columns.\n",
    "\n",
    "Then, the `score_move()` function calculates the value of the heuristic for each valid move. It uses a couple of helper functions:\n",
    "\n",
    "- `drop_piece()` returns the grid that results when the player drops its disc in the selected column.\n",
    "- `get_heuristic()` calculates the value of the heuristic for the supplied board (grid), where mark is the mark of the agent. This function uses the `count_windows()` function, which counts the number of windows (of four adjacent locations in a row, column, or diagonal) that satisfy specific conditions from the heuristic. Specifically, `count_windows(grid, num_discs, piece, config)` yields the number of windows in the game board (grid) that contain num_discs pieces from the player (agent or opponent) with mark `piece`, and where the remaining locations in the window are empty. \n",
    "\n",
    "For instance,\n",
    " - setting `num_discs=4` and `piece=obs.mark` counts the number of times the agent got four discs in a row.\n",
    " - setting `num_discs=3` and `piece=obs.mark%2+1` counts the number of windows where the opponent has three discs, and the remaining location is empty (the opponent wins by filling in the empty spot).\n",
    " \n",
    "Finally, we get the list of columns that maximize the heuristic and select one (uniformly) at random.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([agent, agent_random])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously having included a little bit of logic in our agent has significantly improved it's performance. This agent wins most games and with ease. This is because the agent will calculate what the other player/agent is going to play and play accordingly. Here we see the agent outperform the random agent almost each and every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([agent, agent_leftmost])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, unlike the winner of our previous round, this agent doesn't just play the next valid move, it also looks at the agent (in this case, the agent_leftmost) and if it starts first, it wins, because the agent_leftmost will not try to intervene its play, but if agent_leftmost starts first, then it will interupt it's play, and win in that manner. This will give our agent an edge over agent_leftmost almost all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([agent, agent_middle])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly in this game, most of them are draws as agent_middle will eventually make an invalid move and our agent tries to hinder it's movement. So by the time our agent tries to win, invalid moves would have been played by the agent_middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent, agent2=agent_random, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent, agent2=agent_leftmost, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent, agent2=agent_middle, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see from the results, agent_random wins around 1% of times as there is a chance it might outperform our agent by chamce, while agent_leftmost always loses (100% loss rate in 1000 simulations). Agent 1 has a competitive edge against the agent_middle, but most games have invalid moves being played by agent_middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 N-Stop Lookahead  <a id='n_step'></a>\n",
    "\n",
    "This agent performs reasonably well, but definitely still has room for improvement! For instance, consider the potential moves in the figure below. (Note that we use zero-based numbering for the columns, so the leftmost column corresponds to col=0, the next column corresponds to col=1, and so on.)\n",
    "\n",
    "![title](Images\\N_Step.png)\n",
    "\n",
    "With one-step lookahead, the red player picks one of column 5 or 6, each with 50% probability. But, column 5 is clearly a bad move, as it lets the opponent win the game in only one more turn. Unfortunately, the agent doesn't know this, because it can only look one move into the future.\n",
    "\n",
    "In this tutorial, you'll use the minimax algorithm to help the agent look farther into the future and make better-informed decisions.\n",
    "\n",
    "## Minmax\n",
    "\n",
    "We'd like to leverage information from deeper in the game tree. For now, assume we work with a depth of 3. This way, when deciding its move, the agent considers all possible game boards that can result from\n",
    "\n",
    "1. the agent's move,\n",
    "2. the opponent's move, and\n",
    "3. the agent's next move.\n",
    "\n",
    "We'll work with a visual example. For simplicity, we assume that at each turn, both the agent and opponent have only two possible moves. Each of the blue rectangles in the figure below corresponds to a different game board.\n",
    "\n",
    "![title](Images\\N_Step_.png)\n",
    "\n",
    "We have labeled each of the \"leaf nodes\" at the bottom of the tree with the score from the heuristic. (We use made-up scores in the figure. In the code, we'll use the same heuristic from the previous tutorial.) As before, the current game board is at the top of the figure, and the agent's goal is to end up with a score that's as high as possible.\n",
    "\n",
    "But notice that the agent no longer has complete control over its score -- after the agent makes its move, the opponent selects its own move. And, the opponent's selection can prove disastrous for the agent! In particular,\n",
    "\n",
    "- If the agent chooses the left branch, the opponent can force a score of -1.\n",
    "- If the agent chooses the right branch, the opponent can force a score of +10.\n",
    "\n",
    "Take the time now to check this in the figure, to make sure it makes sense to you!\n",
    "\n",
    "With this in mind, you might argue that the right branch is the better choice for the agent, since it is the less risky option. Sure, it gives up the possibility of getting the large score (+40) that can only be accessed on the left branch, but it also guarantees that the agent gets at least +10 points.\n",
    "\n",
    "This is the main idea behind the **minimax algorithm:** the agent chooses moves to get a score that is as high as possible, and it assumes the opponent will counteract this by choosing moves to force the score to be as low as possible. That is, the agent and opponent have opposing goals, and we assume the opponent plays optimally.\n",
    "\n",
    "So, in practice, how does the agent use this assumption to select a move? We illustrate the agent's thought process in the figure below.\n",
    "\n",
    "![title](Images\\N_Step_eg.png)\n",
    "\n",
    "In the example, minimax assigns the move on the left a score of -1, and the move on the right is assigned a score of +10. So, the agent will select the move on the right.\n",
    "\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to slightly modify the heuristic from the previous tutorial, since the opponent is now able to modify the game board.\n",
    "\n",
    "![title](Images\\N_Step_Heuristic.png)\n",
    "\n",
    "In particular, we need to check if the opponent has won the game by playing a disc. The new heuristic looks at each group of four adjacent locations in a (horizontal, vertical, or diagonal) line and assigns:\n",
    "\n",
    "- 1000000 (1e6) points if the agent has four discs in a row (the agent won),\n",
    "- 1 point if the agent filled three spots, and the remaining spot is empty (the agent wins if it fills in the empty spot),\n",
    "- -100 points if the opponent filled three spots, and the remaining spot is empty (the opponent wins by filling in the empty spot), and\n",
    "- -10000 (-1e4) points if the opponent has four discs in a row (the opponent won).\n",
    "\n",
    "This is defined in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for minimax: calculates value of heuristic for grid\n",
    "def get_heuristic(grid, mark, config):\n",
    "    num_threes = count_windows(grid, 3, mark, config)\n",
    "    num_fours = count_windows(grid, 4, mark, config)\n",
    "    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "    num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n",
    "    score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses minimax to calculate value of dropping piece in selected column\n",
    "def score_move(grid, col, mark, config, nsteps):\n",
    "    next_grid = drop_piece(grid, col, mark, config)\n",
    "    score = minimax(next_grid, nsteps-1, False, mark, config)\n",
    "    return score\n",
    "\n",
    "# Helper function for minimax: checks if agent or opponent has four in a row in the window\n",
    "def is_terminal_window(window, config):\n",
    "    return window.count(1) == config.inarow or window.count(2) == config.inarow\n",
    "\n",
    "# Helper function for minimax: checks if game has ended\n",
    "def is_terminal_node(grid, config):\n",
    "    # Check for draw \n",
    "    if list(grid[0, :]).count(0) == 0:\n",
    "        return True\n",
    "    # Check for win: horizontal, vertical, or diagonal\n",
    "    # horizontal \n",
    "    for row in range(config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[row, col:col+config.inarow])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # vertical\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns):\n",
    "            window = list(grid[row:row+config.inarow, col])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # positive diagonal\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # negative diagonal\n",
    "    for row in range(config.inarow-1, config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Minimax implementation\n",
    "def minimax(node, depth, maximizingPlayer, mark, config):\n",
    "    is_terminal = is_terminal_node(node, config)\n",
    "    valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n",
    "    if depth == 0 or is_terminal:\n",
    "        return get_heuristic(node, mark, config)\n",
    "    if maximizingPlayer:\n",
    "        value = -np.Inf\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(node, col, mark, config)\n",
    "            value = max(value, minimax(child, depth-1, False, mark, config))\n",
    "        return value\n",
    "    else:\n",
    "        value = np.Inf\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(node, col, mark%2+1, config)\n",
    "            value = min(value, minimax(child, depth-1, True, mark, config))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deeper we make the game tree, the longer it will take to run. So here, we make it 3 steps deep. That should give our agent enough competitive edge even against the finest of human players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How deep to make the game tree: higher values take longer to run!\n",
    "N_STEPS = 3\n",
    "\n",
    "def agentN(obs, config):\n",
    "    # Get list of valid moves\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Convert the board to a 2D grid\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Select at random from the maximizing columns\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([agentN, agent_random])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our agent annihilates the agent_random, but it looks 3 steps ahead just to make sure that random_agent doesn't have a chance to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([agentN, agent_leftmost])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it performs very well against the agent_leftmost. And since it takes a look at 3-steps ahead, what we might be thinking as taking extra time to win the game, the agent is seeing as a strategy to solidify its victory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([agentN, agent_middle])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent_middle loses out again because of the invalid moves it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 1\n",
    "\n",
    "def agentO(obs, config):\n",
    "    # Get list of valid moves\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Convert the board to a 2D grid\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Select at random from the maximizing columns\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two random agents play one game round\n",
    "env.run([agentN, agentO])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_win_percentages(agent1=agentN, agent2=agent_random, n_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_win_percentages(agent1=agentN, agent2=agent_leftmost, n_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "get_win_percentages(agent1=agentN, agent2=agent_middle, n_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_win_percentages(agent1=agentN, agent2=agentO, n_rounds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the agentN (N-step lookahead), wins against all three, random, leftmost and middle, with middle playing the most illegal moves.\n",
    "\n",
    "Interestingly enough, one-step and n-step have a close competition with win rates being around 50% for each. Maybe N-step is trying to overplay or overestimate how good one-step is. But this is surely an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Beat the Bot  <a id='beat_the_bot'></a>\n",
    "\n",
    "Try playing against the AI yourself ;) You can change the difficulty levels by changing the agents.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.play([agentN, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can have a go at trying to defeat the AI by playing a game against it. You can specify which agent you want to play against by specifying in the column above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Conclusion<p><a id='Conclusion'></a>\n",
    "    \n",
    "1. When we play random agent against random agent, the result, as expected is random.\n",
    "2. When we try to introduce rudimentary logic like leftmost valid or middle, it has a significant buff against random agents, but its still nowhere near human player.\n",
    "3. When we introduce simple logic and foresight of looking, one-step ahead, its performance is significantly better against all three (random, leftmost and middle).\n",
    "4. N-step agent performs the best against all the previous agents. It even outperforms many human players.\n",
    "5. One anomaly is that N-step and One-step have a close competition while we expect N-step to outperform One-step all the time.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Contribution<p><a id='Contribution'></a>\n",
    "\n",
    "This was a fun project in which we explore the idea of Reinforcement Learning. \n",
    "       \n",
    "- Code by self : 45%\n",
    "- Code from external Sources : 55%\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Citation<p><a id='Citation'></a>\n",
    "- https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/\n",
    "- https://towardsdatascience.com/the-ultimate-beginners-guide-to-reinforcement-learning-588c071af1ec\n",
    "- https://medium.com/@zsalloum/basics-of-reinforcement-learning-the-easy-way-fb3a0a44f30e\n",
    "- https://www.kaggle.com/alexisbcook/play-the-game\n",
    "- https://www.kaggle.com/alexisbcook/one-step-lookahead\n",
    "- https://www.kaggle.com/alexisbcook/n-step-lookahead\n",
    "- https://www.kaggle.com/c/connectx/overview\n",
    "- https://deeplizard.com\n",
    "    \n",
    "[Back to top](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">License<p><a id='License'></a>\n",
    "Copyright (c) 2020 Manali Sharma, Rushabh Nisher\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "[Back to top](#Introduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
