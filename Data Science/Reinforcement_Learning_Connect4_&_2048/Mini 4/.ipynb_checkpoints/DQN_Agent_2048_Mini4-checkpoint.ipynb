{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://thumbs.gfycat.com/ElegantSaneKingbird-max-14mb.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Table of Contents </p>\n",
    "\n",
    "- ## 1. [Introduction](#Introduction)\n",
    "   - ### 1.1 [Abstract](#abstract)\n",
    "   - ### 1.2 [Importing Libraries](#import)\n",
    "- ## 2 [Game-2048](#en)\n",
    "   - ### 2.1 [Q-Learning](#q)\n",
    "   - ### 2.2 [Why Deep Q-Learning?](#wq)\n",
    "   - ### 2.3 [Deep Q-Network](#deepq)\n",
    "   - ### 2.4 [Challenges in Deep RL as Compared to Deep Learning](#cc)\n",
    "       - #### 2.4.1 [Target Network](#tr)\n",
    "       - #### 2.4.2 [Experience Replay](#er)\n",
    "       - #### 2.4.3 [Putting it together](#pt)\n",
    "   - ### 2.5 [Implementation](#i)\n",
    "       - #### 2.5.1 [Parameters](#par)\n",
    "       - #### 2.5.2 [Artificial Neural Network](#ar)\n",
    "       - #### 2.5.3 [Remeber](#rer)\n",
    "       - #### 2.5.4 [Game Step](#gs)\n",
    "       - #### 2.5.5 [Replay](#rep)\n",
    "       - #### 2.5.6 [Putting it all together](#ptt)\n",
    "       - #### 2.5.7 [Train](#tr)\n",
    "       - #### 2.5.8 [Plotting](#ptl)\n",
    "       - #### 2.5.9 [Results](#ree)\n",
    "       - #### 2.5.10 [Playing the game](#pgg)\n",
    "       \n",
    "       \n",
    "- ## 3. [Conclusion](#Conclusion)\n",
    "- ## 4. [Contribution](#Contribution)\n",
    "- ## 5. [Citation](#Citation)\n",
    "- ## 6. [License](#License)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='Introduction'> 1. Introduction </a>\n",
    "\n",
    "## <a id=\"abstract\"> 1.1 Abstract </a>\n",
    "\n",
    "This repository trains a q deep learning network from the game 2048 and plots a performance graph. The gamelogic of the game 2048 is based on the implementation from Georg Wiese on his GitHub Repo . The deep q learning code is loosely based on the implementation form this GitHub Repo tutorial and was greatly enhanced and adapted to include the game 2048.\n",
    "\n",
    "\n",
    "## <a id=\"import\"> 1.2 Importing Libaries </a>\n",
    "\n",
    "<br>First, we import the libraries and the gamelogic class. As mentioned before, the whole code can be run in the section \"Train it\" at the end of this description. The dependencies are also listed in requirements.txt for an easy install with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import json\n",
    "import time\n",
    "from shutil import copyfile\n",
    "import parameters\n",
    "import os\n",
    "#from gamelogic.game Game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"en\">2. Game-2048 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2048 is a single-player sliding block puzzle game developed by Gabriele Cirulli in 2014. The game represents a 4 × 4 grid where the value of each cell is a power of 2. An action can be any of the 4 movements: up, down, left right. When an action is performed, all cells move in the chosen direction. Any two adjacent cells with the same value (power of 2) along this direction merge to form one single cell with value equal to the sum of the two cells (i.e. the next power of 2). The objective of the game is to combine cells until reaching 2048. After each move, a new tile appears at a random empty cell. The game is finished/lost if all cells are full.\n",
    "To get a quick feeling of the game it is recommended to check out the free online version [here](http://2048game.com/).\n",
    "\n",
    "The game itself (gamelogic) of the game 2048 can be found in the folder gamelogic in file game.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies\n",
    "2048 is a game which starts easy but becomes very hard for human players. Obviously merging tiles whenever you can will get you to a certain level, but to reach a high score (or maximum value of e.g. 2048) there needs to be more sophisticated strategies. One of the most famous is to put the highest numbers in one corner (\"corner-strategy\"), like it is explained  [here](https://www.cnet.com/news/2048-starts-easy-gets-hard-heres-how-to-make-it-easy-again/). This technique also used by the most successfull 2048 AIs, for example [this one](http://www.randalolson.com/2015/04/27/artificial-intelligence-has-crushed-all-human-records-in-2048-heres-how-the-ai-pulled-it-off/) using an expectimax algorithm. \n",
    "The challenge of this game for AI is the high amount of possible states (more than 16^12) combined with the randomness introduced when spawning the new tiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"q\"> 2.1 Q-Learning </a>\n",
    "\n",
    "Let’s say we know the expected reward of each action at every step. This would essentially be like a cheat sheet for the agent! Our agent will know exactly which action to perform.\n",
    "\n",
    "It will perform the sequence of actions that will eventually generate the maximum total reward. This total reward is also called the Q-value and we will formalise our strategy as:\n",
    "\n",
    "![](1.png)\n",
    "\n",
    "\n",
    "The above equation states that the Q-value yielded from being at state s and performing action a is the immediate reward r(s,a) plus the highest Q-value possible from the next state s’. Gamma here is the discount factor which controls the contribution of rewards further in the future.\n",
    "\n",
    "Q(s’,a) again depends on Q(s”,a) which will then have a coefficient of gamma squared. So, the Q-value depends on Q-values of future states as shown here:\n",
    "\n",
    "![](2.png)\n",
    "\n",
    "Adjusting the value of gamma will diminish or increase the contribution of future rewards.\n",
    "\n",
    "Since this is a recursive equation, we can start with making arbitrary assumptions for all q-values. With experience, it will converge to the optimal policy. In practical situations, this is implemented as an update:\n",
    "\n",
    "![](3.png)\n",
    "\n",
    "where alpha is the learning rate or step size. This simply determines to what extent newly acquired information overrides old information.\n",
    "\n",
    "\n",
    "\n",
    "#### Reward\n",
    "We tried both the score and the maximum value of the tile as the reward value. The score is calculated in a way that it increases every time by the value of the newly merged tiles. If for example two 4s are merged the reward is 8.\n",
    "This means the score partly represents the highest value on the board but gives also an incentive to have multiple high value tiles compared to just looking at the maximum value present at the board.\n",
    "\n",
    "For our algorithm, we implemented both versions of the reward, which results in a optimization of the score or a win-lose classification with a fixed target respectively.\n",
    "\n",
    "As the results below show, we achieved similar results with both approaches.\n",
    "\n",
    "#### Loss\n",
    "In order to logically represent this intuition and train it, we need to express this as a formula that we can optimize on. The loss is just a value that indicates how far our prediction is from the actual target. For example, the prediction of the model could indicate that it sees more value in swiping left when in fact it can gain more reward by swiping upwards. We want to decrease this gap between the prediction and the target (loss). The loss is defined as:\n",
    "\n",
    "$$ L = \\frac{1}{2} \\underbrace{\t[r + \\gamma*  max_{a'}Q(s',a')}_{\\mathrm{target}} -\\underbrace{Q(s,a)}_{\\mathrm{prediction}}]^2 $$\n",
    "\n",
    "whith the target q value looking like this in the code: <br>\n",
    "target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "\n",
    "## <a id=\"wq\"> 2.2 Why ‘Deep’ Q-Learning? </a>\n",
    "\n",
    "Q-learning is a simple yet quite powerful algorithm to create a cheat sheet for our agent. This helps the agent figure out exactly which action to perform.\n",
    "\n",
    "But what if this cheatsheet is too long? Imagine an environment with 10,000 states and 1,000 actions per state. This would create a table of 10 million cells. Things will quickly get out of control!\n",
    "\n",
    "It is pretty clear that we can’t infer the Q-value of new states from already explored states. This presents two problems:\n",
    "\n",
    "First, the amount of memory required to save and update that table would increase as the number of states increases\n",
    "Second, the amount of time required to explore each state to create the required Q-table would be unrealistic\n",
    "\n",
    "## <a id=\"deepq\">2.3 Deep Q-Networks </a>\n",
    "\n",
    "In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output. The comparison between Q-learning & deep Q-learning is wonderfully illustrated below:\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM-850x558.png)\n",
    "\n",
    "So, what are the steps involved in reinforcement learning using deep Q-learning networks (DQNs)?\n",
    "\n",
    "#### - 1.All the past experience is stored by the user in memory\n",
    "#### - 2. The next action is determined by the maximum output of the Q-network\n",
    "#### - 3. The loss function here is mean squared error of the predicted Q-value and the target Q-value – Q*. This is basically a regression problem. However, we do not know the target or actual value here as we are dealing with a reinforcement learning problem. Going back to the Q-value update equation derived fromthe Bellman equation. we have:\n",
    "\n",
    "![](4.png)\n",
    "\n",
    "`The section in green represents the target. We can argue that it is predicting its own value, but since R is the unbiased true reward, the network is going to update its gradient using backpropagation to finally converge.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"cc\"> 2.4 Challenges in Deep RL as Compared to Deep Learning </a>\n",
    "\n",
    "- #### Non-stationary or unstable target: Let us go back to the pseudocode for deep Q-learning:\n",
    "![](5.png)\n",
    "As you can see in the above code, the target is continuously changing with each iteration. In deep learning, the target variable does not change and hence the training is stable, which is just not true for RL.\n",
    "\n",
    "To summarise, we often depend on the policy or value functions in reinforcement learning to sample actions. However, this is frequently changing as we continuously learn what to explore. As we play out the game, we get to know more about the ground truth values of states and actions and hence, the output is also changing.\n",
    "\n",
    "### <a id=\"tr\"> 2.4.1 Target Network</a> \n",
    "Since the same network is calculating the predicted value and the target value, there could be a lot of divergence between these two. So, instead of using 1one neural network for learning, we can use two.\n",
    "\n",
    "We could use a separate network to estimate the target. This target network has the same architecture as the function approximator but with frozen parameters. For every C iterations (a hyperparameter), the parameters from the prediction network are copied to the target network. This leads to more stable training because it keeps the target function fixed (for a while):\n",
    "\n",
    "![](6.png)\n",
    "\n",
    "### <a id=\"er\">2.4.2 Experience Replay</a>\n",
    "\n",
    "![](7.png)\n",
    "What does the above statement mean? Instead of running Q-learning on state/action pairs as they occur during simulation or the actual experience, the system stores the data discovered for [state, action, reward, next_state] – in a large table.\n",
    "\n",
    "Let’s understand this using an example.\n",
    "\n",
    "Suppose we are trying to build a video game bot where each frame of the game represents a different state. During training, we could sample a random batch of 64 frames from the last 100,000 frames to train our network. This would get us a subset within which the correlation amongst the samples is low and will also provide better sampling efficiency.\n",
    "\n",
    "### <a id=\"pt\"> 2.4.3 Putting it all Together </a>\n",
    "\n",
    "The concepts we have learned so far? They all combine to make the deep Q-learning algorithm that was used to achive human-level level performance in Atari games (using just the video frames of the game).\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-17-at-1.15.28-PM.png)\n",
    "\n",
    "#### we have listed the steps involved in a deep Q-network (DQN) below:\n",
    "\n",
    "> - 1. Preprocess and feed the game screen (state s) to our DQN, which will return the Q-values of all possible actions in the state\n",
    "> - 2. Select an action using the epsilon-greedy policy. With the probability epsilon, we select a random action a and with probability 1-epsilon, we select an action that has a maximum Q-value, such as a = argmax(Q(s,a,w))\n",
    ">- 3. Perform this action in a state s and move to a new state s’ to receive a reward. This state s’ is the preprocessed image of the next game screen. We store this transition in our replay buffer as <s,a,r,s’>\n",
    ">- 4. Next, sample some random batches of transitions from the replay buffer and calculate the loss\n",
    ">- 5.It is known that: which is just the squared difference between target Q and predicted Q\n",
    ">- 6. Perform gradient descent with respect to our actual network parameters in order to minimize this loss\n",
    ">- 7. After every C iterations, copy our actual network weights to the target network weights\n",
    ">- 8. Repeat these steps for M number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"i\"> 2.5 Implementation </a> \n",
    "The repo consists of two parts: the learning part and the full programmed game of 2048.\n",
    "The gamelogic of the game 2048 can be found in the folder gamelogic in file game.py.\n",
    "The code can be run on both python 2.7 and 3.5 versions. \n",
    "\n",
    "###  <a id=\"par\"> 2.5.1 Parameters </a>\n",
    "Next, we define the number of episodes to play and set the hyperparameters.<br>\n",
    "The parameters can be set in a seperate file called parameters.py.\n",
    "Every 100 episodes, key values are being saved in the folder data and can be plotted with the file plot.py.<br>\n",
    "We implemented an epsilon greedy algorithm with a decay. As you can see, the agent only explores in the beginning and starts decaying at a given rate epsilon_decay.<br>\n",
    "\n",
    "The hyperparameters used are explained below and an example is given for a good configuration using 100'000 episodes.<br>\n",
    "\n",
    "\n",
    "<b>gamma</b> = 0.001<br>\n",
    "In the case of taking the highest maximum value as the reward, gamma can be set to be very low or even to 0 because it is very hard to reach a reward.\n",
    "In the case of the score set as the reward where there are intermediate rewards, we found it to be better if gamma is not zero.\n",
    "\n",
    "<b>epsilon_decay</b> = 0.99992 <br>\n",
    "Epsilon decay determines how fast epsilon decays.\n",
    "We found 0.99992 is an apropriate amount, because it reaches the min_epsilon=0.01 after 60'000 episodes and our computing power restricts us to max 100'000 episodes.<br>\n",
    "\n",
    "<b>learning_rate</b> = 0.01 <br>\n",
    "The learning_rate determines how fast gradient descent will find the solution. A too large learning rate will lead to overshooting and not finding the optimal solution.\n",
    "We had the best results with a standard learning_rate of 0.01, but there was no significant difference in the outcome if we changed it to 0.001.\n",
    "\n",
    "<b>batch_size</b> = 32 <br>\n",
    "This determines the size of the batches used to replay and therefore to train the network.\n",
    "We found 32 to be used in most comparable problems. A batch size of 64 for instance would slow down training too much without any improvements.\n",
    "\n",
    "<b>is_max_value_reward </b>= True<br>\n",
    "This is the boolean value which can be set to either using the score (false) or the maximum value as a reward (true).<br>\n",
    "It can be seen below, both of the reward structures gave us similar results altough intuitively we expected the score to perform better as it gives more intermediate reward and there is no fixed threshold so we expected it to learn faster.\n",
    "\n",
    "<b>max_value_reward_threshold</b> = 8<br>\n",
    "This parameter determines the threshold which number the agent has to reach to get the reward. \n",
    "If this is set to 8, the agent has to reach a 2^9=512 tile to get the reward.<br>\n",
    "As it takes a very long time to reach 2^10 but 2^9 is reached rather quickly those are the two configuratios we used. 2^9 (parameter of 8) performed much better then 2^10. We assume this is because 1024 is not reached enough times and because of the randomness of the game it is hard to replicate successful runs. \n",
    "\n",
    "<b>max_value_reward_amount</b> = 1000<br>\n",
    "This is only used if is_max_value_reward = True and determines the reward which the agent gets when reaching the threshold. Because discount rate is very small and there are no other rewards, this parameter does not have a big influence on the result.\n",
    "\n",
    "This is where the parameter.py file is feeded into the agent: <br>\n",
    "```python\n",
    "EPISODES = 1000\n",
    "\n",
    "path  = os.getcwd()\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = 16\n",
    "        self.action_size = 4 # (up, down, right, left)\n",
    "        self.memory = deque(maxlen=5000000)\n",
    "        self.gamma = parameters.gamma    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = parameters.epsilon_decay\n",
    "        self.learning_rate = parameters.learning_rate\n",
    "        self.model = self._build_model()\n",
    "        self.batch_size = parameters.batch_size\n",
    "        self.is_max_value_reward = parameters.is_max_value_reward\n",
    "        self.max_value_reward_threshold = parameters.max_value_reward_threshold\n",
    "        self.max_value_reward_amount = parameters.max_value_reward_amount\n",
    "        self.output_name = parameters.output_name\n",
    "```\n",
    "\n",
    "### <a i=\"ar\"> 2.5.2 Artificial Neural Network </a>\n",
    "\n",
    "The deep network is a standard artificial neural network consisting of two fully connected hidden layers with 256 nodes each. As activation functions ReLu was used for all layers, which guarantees non vanishing gradients. The loss was computed using the mean squared error (mse). Bigger losses are therefore punished more. As optimizer we used Adam.\n",
    "\n",
    "Keras does all the work of subtracting the target from the neural network output and squaring it. It also applies the learning rate we defined while creating the neural network model. This all happens inside the fit() function we will see later. This function decreases the gap between our prediction to target by the learning rate. The approximation of the Q-value converges to the true Q-value as we repeat the updating process. The loss will decrease and the score will go up.\n",
    "\n",
    "```python\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='relu'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "```\n",
    "        \n",
    "### <a id=\"rer\"> 2.5.3 Remember </a>\n",
    "\n",
    "One of the challenges for DQN is that neural network used in the algorithm tends to forget the previous experiences as it overwrites them with new experiences. So we implemented a replay memory, stored as a list of previous experiences and observations to train the model with the previous experiences. We will call this array of experiences memory and use the remember() function to append state, action, reward, and next state to the memory.\n",
    "\n",
    "In our example, the memory list will have a form of:\n",
    "```python\n",
    "memory = [(state, action, reward, next_state, done)]\n",
    "```\n",
    "\n",
    "\n",
    "The remember function will simply store states, actions and resulting rewards to the memory like below:\n",
    "\n",
    "```python\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "```\n",
    "\n",
    "\n",
    "### <a id=\"gs\"> 2.5.4 Game Step </a>\n",
    "\n",
    "The act method returns the optimal action by feeding the state through our neural network. First we implement the epsilon-greedy algorithm and get the four Q-values (which are the output nodes of our neural network) associated with the four possible actions we can do in this move. We then compare these action values with the possible actions, since sometimes we are limited in the actions we can take. We choose the action with the highest Q value that we are allowed to take.\n",
    "\n",
    "```python\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(game.available_actions())\n",
    "        #forward feeding\n",
    "        act_values = self.model.predict(state)\n",
    "        #temporarily sets q-values of not available actions to -100 so they are not chosen\n",
    "        if len(game.available_actions())< 4:\n",
    "          temp = game.available_actions()\n",
    "          for i in range(0, 4):\n",
    "            if i not in temp:\n",
    "              act_values[0][i] = -100\n",
    "        #returns action with highest q-value\n",
    "        return np.argmax(act_values[0])\n",
    "        #replace return with this for a random agent:\n",
    "        #return random.choice(game.available_actions())\n",
    "```\n",
    "\n",
    "### <a id=\"rep\">2.5.5 Replay </a>\n",
    "\n",
    "The replay function trains the neural network with experience from the memory. It first samples a minibatch from the memory. Each memory contains the current state, action, next state and its reward and a boolean done of each state of the minibatch, indicating whether the game is over.\n",
    "The Q learning algorithm is implemented as:\n",
    "```python\n",
    "target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "```\n",
    "where self.model.predict(next_state)[0] returns the Q-value of the next_state.\n",
    "\n",
    "```python\n",
    "self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "```\n",
    "trains one epoch by calculating the loss between the target q value and the predicted q value.\n",
    "Finally, we apply epsilon decay.\n",
    "\n",
    "```python\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"trains the neural net with experiences from memory (minibatches)\"\"\"\n",
    "        #samples mimibatch from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        #for each memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #if its final state set target to the reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                #set target according to formula\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            #gets all 4 predictions from current state\n",
    "            target_f = self.model.predict(state)\n",
    "            #takes the one action which was selected in batch\n",
    "            target_f[0][action] = target\n",
    "            #trains the model\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "```\n",
    "\n",
    "#### Loading and saving the weights\n",
    "The weights can be loaded and saved, so training can be interrupted and continued.\n",
    "\n",
    "```python\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### <a id=\"ptt\"> 2.5.6 Putting it all together </a>\n",
    "The main function loops through the episodes. First we reset the game and get the start state:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        game.new_game()\n",
    "        state = game.state()\n",
    "        state = np.reshape(state, [1, agent.state_size])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As long as the game is not over, the act function get's called to receive the calculated action and the reward gets calculated. If is_max_value_reward is set to TRUE the reward gets calculated by getting the value of the maximum tile. We get this by looking up the highest value from the state variable, which is the playing field containing all tiles represented as a vector. <br>\n",
    "If is_max_value_reward is set to FALSE it takes the squared score as reward. The boolean \"done\" checks, whether the game is over and breaks the loop if so, to continue to the next episode:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "  while not game.game_over():\n",
    "            action = agent.act(state)\n",
    "            reward = (game.do_action(action))**2\n",
    "            if(agent.is_max_value_reward):\n",
    "                reward = 0\n",
    "                temp = game.state()\n",
    "                temp_reshaped = np.reshape(temp, [1, agent.state_size])\n",
    "                temp_max_value = np.amax(temp_reshaped[0])\n",
    "                if temp_max_value > agent.max_value_reward_threshold:\n",
    "                    reward = agent.max_value_reward_amount\n",
    "            next_state = game.state()\n",
    "            actions_available = game.available_actions()\n",
    "            if len(actions_available) == 0: \n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "            next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                if (debug): print(\"no action available\")\n",
    "                states = game.state()\n",
    "                states = np.reshape(state, [1, agent.state_size])\n",
    "                max_value = np.amax(states[0])\n",
    "                output_list.append([e, np.asscalar(max_value), np.asscalar(game.score()), agent.epsilon])\n",
    "                if(debug):print(\"max_value: \" + str(max_value))\n",
    "                break\n",
    "        print(\"episodes: \" + str(e))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For plotting, we save the different parameters together with all the data into a json file:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "        if save_maxvalues:\n",
    "            \n",
    "            if e == 100:\n",
    "                output_list.insert(0, \"gamma: \"+str(parameters.gamma)+\" | epsilon decay: \"+str(parameters.epsilon_decay)+\" | learning rate: \"+str(parameters.learning_rate)+\"\\n batch size: \"+str(parameters.batch_size)+\" | reward = maxVal: \"+str(parameters.is_max_value_reward)+\" | reward amount: \"+str(parameters.max_value_reward_amount)+\" | reward threshold: \"+str(parameters.max_value_reward_threshold))\n",
    "            if e % 100 == 0:\n",
    "                with open(path + \"/data/\"+agent.output_name+\"output.txt\", \"w\") as outfile:\n",
    "                    json.dump(output_list, outfile)\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "```\n",
    "\n",
    "Each 1000 episode, the weights are being saved in the data folder to be able to continue training and to play a game from this checkpoint.\n",
    "\n",
    "```python               \n",
    "        if e % 1000 == 0:\n",
    "            timenow = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            savepath = path + \"/data/agent\"+agent.output_name+timenow+\"_Epi\"+str(e)\n",
    "            agent.save(savepath)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://emerj.com/wp-content/uploads/2018/04/3049155-poster-p-1-machine-learning-is-just-a-big-game-of-plinko.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"tt\"> 2.5.7 Train </a>\n",
    "Just execute the following cell. Can be interrupted at anytime, since values are being stored for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes: 0\n",
      "episodes: 1\n",
      "episodes: 2\n",
      "episodes: 3\n",
      "episodes: 4\n",
      "episodes: 5\n",
      "episodes: 6\n",
      "episodes: 7\n",
      "episodes: 8\n",
      "episodes: 9\n",
      "episodes: 10\n",
      "episodes: 11\n",
      "episodes: 12\n",
      "episodes: 13\n",
      "episodes: 14\n",
      "episodes: 15\n",
      "episodes: 16\n",
      "episodes: 17\n",
      "episodes: 18\n",
      "episodes: 19\n",
      "episodes: 20\n",
      "episodes: 21\n",
      "episodes: 22\n",
      "episodes: 23\n",
      "episodes: 24\n",
      "episodes: 25\n",
      "episodes: 26\n",
      "episodes: 27\n",
      "episodes: 28\n",
      "episodes: 29\n",
      "episodes: 30\n",
      "episodes: 31\n",
      "episodes: 32\n",
      "episodes: 33\n",
      "episodes: 34\n",
      "episodes: 35\n",
      "episodes: 36\n",
      "episodes: 37\n",
      "episodes: 38\n",
      "episodes: 39\n",
      "episodes: 40\n",
      "episodes: 41\n",
      "episodes: 42\n",
      "episodes: 43\n",
      "episodes: 44\n",
      "episodes: 45\n",
      "episodes: 46\n",
      "episodes: 47\n",
      "episodes: 48\n",
      "episodes: 49\n",
      "episodes: 50\n",
      "episodes: 51\n",
      "episodes: 52\n",
      "episodes: 53\n",
      "episodes: 54\n",
      "episodes: 55\n",
      "episodes: 56\n",
      "episodes: 57\n",
      "episodes: 58\n",
      "episodes: 59\n",
      "episodes: 60\n",
      "episodes: 61\n",
      "episodes: 62\n",
      "episodes: 63\n",
      "episodes: 64\n",
      "episodes: 65\n",
      "episodes: 66\n",
      "episodes: 67\n",
      "episodes: 68\n",
      "episodes: 69\n",
      "episodes: 70\n",
      "episodes: 71\n",
      "episodes: 72\n",
      "episodes: 73\n",
      "episodes: 74\n",
      "episodes: 75\n",
      "episodes: 76\n",
      "episodes: 77\n",
      "episodes: 78\n",
      "episodes: 79\n",
      "episodes: 80\n",
      "episodes: 81\n",
      "episodes: 82\n",
      "episodes: 83\n",
      "episodes: 84\n",
      "episodes: 85\n",
      "episodes: 86\n",
      "episodes: 87\n",
      "episodes: 88\n",
      "episodes: 89\n",
      "episodes: 90\n",
      "episodes: 91\n",
      "episodes: 92\n",
      "episodes: 93\n",
      "episodes: 94\n",
      "episodes: 95\n",
      "episodes: 96\n",
      "episodes: 97\n",
      "episodes: 98\n",
      "episodes: 99\n",
      "episodes: 100\n",
      "episodes: 101\n",
      "episodes: 102\n",
      "episodes: 103\n",
      "episodes: 104\n",
      "episodes: 105\n",
      "episodes: 106\n",
      "episodes: 107\n",
      "episodes: 108\n",
      "episodes: 109\n",
      "episodes: 110\n",
      "episodes: 111\n",
      "episodes: 112\n",
      "episodes: 113\n",
      "episodes: 114\n",
      "episodes: 115\n",
      "episodes: 116\n",
      "episodes: 117\n",
      "episodes: 118\n",
      "episodes: 119\n",
      "episodes: 120\n",
      "episodes: 121\n",
      "episodes: 122\n",
      "episodes: 123\n",
      "episodes: 124\n",
      "episodes: 125\n",
      "episodes: 126\n",
      "episodes: 127\n",
      "episodes: 128\n",
      "episodes: 129\n",
      "episodes: 130\n",
      "episodes: 131\n",
      "episodes: 132\n",
      "episodes: 133\n",
      "episodes: 134\n",
      "episodes: 135\n",
      "episodes: 136\n",
      "episodes: 137\n",
      "episodes: 138\n",
      "episodes: 139\n",
      "episodes: 140\n",
      "episodes: 141\n",
      "episodes: 142\n",
      "episodes: 143\n",
      "episodes: 144\n",
      "episodes: 145\n",
      "episodes: 146\n",
      "episodes: 147\n",
      "episodes: 148\n",
      "episodes: 149\n",
      "episodes: 150\n",
      "episodes: 151\n",
      "episodes: 152\n",
      "episodes: 153\n",
      "episodes: 154\n",
      "episodes: 155\n",
      "episodes: 156\n",
      "episodes: 157\n",
      "episodes: 158\n",
      "episodes: 159\n",
      "episodes: 160\n",
      "episodes: 161\n",
      "episodes: 162\n",
      "episodes: 163\n",
      "episodes: 164\n",
      "episodes: 165\n",
      "episodes: 166\n",
      "episodes: 167\n",
      "episodes: 168\n",
      "episodes: 169\n",
      "episodes: 170\n",
      "episodes: 171\n",
      "episodes: 172\n",
      "episodes: 173\n",
      "episodes: 174\n",
      "episodes: 175\n",
      "episodes: 176\n",
      "episodes: 177\n",
      "episodes: 178\n",
      "episodes: 179\n",
      "episodes: 180\n",
      "episodes: 181\n",
      "episodes: 182\n",
      "episodes: 183\n",
      "episodes: 184\n",
      "episodes: 185\n",
      "episodes: 186\n",
      "episodes: 187\n",
      "episodes: 188\n",
      "episodes: 189\n",
      "episodes: 190\n",
      "episodes: 191\n",
      "episodes: 192\n",
      "episodes: 193\n",
      "episodes: 194\n",
      "episodes: 195\n",
      "episodes: 196\n",
      "episodes: 197\n",
      "episodes: 198\n",
      "episodes: 199\n",
      "episodes: 200\n",
      "episodes: 201\n",
      "episodes: 202\n",
      "episodes: 203\n",
      "episodes: 204\n",
      "episodes: 205\n",
      "episodes: 206\n",
      "episodes: 207\n",
      "episodes: 208\n",
      "episodes: 209\n",
      "episodes: 210\n",
      "episodes: 211\n",
      "episodes: 212\n",
      "episodes: 213\n",
      "episodes: 214\n",
      "episodes: 215\n",
      "episodes: 216\n",
      "episodes: 217\n",
      "episodes: 218\n",
      "episodes: 219\n",
      "episodes: 220\n",
      "episodes: 221\n",
      "episodes: 222\n",
      "episodes: 223\n",
      "episodes: 224\n",
      "episodes: 225\n",
      "episodes: 226\n",
      "episodes: 227\n",
      "episodes: 228\n",
      "episodes: 229\n",
      "episodes: 230\n",
      "episodes: 231\n",
      "episodes: 232\n",
      "episodes: 233\n",
      "episodes: 234\n",
      "episodes: 235\n",
      "episodes: 236\n",
      "episodes: 237\n",
      "episodes: 238\n",
      "episodes: 239\n",
      "episodes: 240\n",
      "episodes: 241\n",
      "episodes: 242\n",
      "episodes: 243\n",
      "episodes: 244\n",
      "episodes: 245\n",
      "episodes: 246\n",
      "episodes: 247\n",
      "episodes: 248\n",
      "episodes: 249\n",
      "episodes: 250\n",
      "episodes: 251\n",
      "episodes: 252\n",
      "episodes: 253\n",
      "episodes: 254\n",
      "episodes: 255\n",
      "episodes: 256\n",
      "episodes: 257\n",
      "episodes: 258\n",
      "episodes: 259\n",
      "episodes: 260\n",
      "episodes: 261\n",
      "episodes: 262\n",
      "episodes: 263\n",
      "episodes: 264\n",
      "episodes: 265\n",
      "episodes: 266\n",
      "episodes: 267\n",
      "episodes: 268\n",
      "episodes: 269\n",
      "episodes: 270\n",
      "episodes: 271\n",
      "episodes: 272\n",
      "episodes: 273\n",
      "episodes: 274\n",
      "episodes: 275\n",
      "episodes: 276\n",
      "episodes: 277\n",
      "episodes: 278\n",
      "episodes: 279\n",
      "episodes: 280\n",
      "episodes: 281\n",
      "episodes: 282\n",
      "episodes: 283\n",
      "episodes: 284\n",
      "episodes: 285\n",
      "episodes: 286\n",
      "episodes: 287\n",
      "episodes: 288\n",
      "episodes: 289\n",
      "episodes: 290\n",
      "episodes: 291\n",
      "episodes: 292\n",
      "episodes: 293\n",
      "episodes: 294\n",
      "episodes: 295\n",
      "episodes: 296\n",
      "episodes: 297\n",
      "episodes: 298\n",
      "episodes: 299\n",
      "episodes: 300\n",
      "episodes: 301\n",
      "episodes: 302\n",
      "episodes: 303\n",
      "episodes: 304\n",
      "episodes: 305\n",
      "episodes: 306\n",
      "episodes: 307\n",
      "episodes: 308\n",
      "episodes: 309\n",
      "episodes: 310\n",
      "episodes: 311\n",
      "episodes: 312\n",
      "episodes: 313\n",
      "episodes: 314\n",
      "episodes: 315\n",
      "episodes: 316\n",
      "episodes: 317\n",
      "episodes: 318\n",
      "episodes: 319\n",
      "episodes: 320\n",
      "episodes: 321\n",
      "episodes: 322\n",
      "episodes: 323\n",
      "episodes: 324\n",
      "episodes: 325\n",
      "episodes: 326\n",
      "episodes: 327\n",
      "episodes: 328\n",
      "episodes: 329\n",
      "episodes: 330\n",
      "episodes: 331\n",
      "episodes: 332\n",
      "episodes: 333\n",
      "episodes: 334\n",
      "episodes: 335\n",
      "episodes: 336\n",
      "episodes: 337\n",
      "episodes: 338\n",
      "episodes: 339\n",
      "episodes: 340\n",
      "episodes: 341\n",
      "episodes: 342\n",
      "episodes: 343\n",
      "episodes: 344\n",
      "episodes: 345\n",
      "episodes: 346\n",
      "episodes: 347\n",
      "episodes: 348\n",
      "episodes: 349\n",
      "episodes: 350\n",
      "episodes: 351\n",
      "episodes: 352\n",
      "episodes: 353\n",
      "episodes: 354\n",
      "episodes: 355\n",
      "episodes: 356\n",
      "episodes: 357\n",
      "episodes: 358\n",
      "episodes: 359\n",
      "episodes: 360\n",
      "episodes: 361\n",
      "episodes: 362\n",
      "episodes: 363\n",
      "episodes: 364\n",
      "episodes: 365\n",
      "episodes: 366\n",
      "episodes: 367\n",
      "episodes: 368\n",
      "episodes: 369\n",
      "episodes: 370\n",
      "episodes: 371\n",
      "episodes: 372\n",
      "episodes: 373\n",
      "episodes: 374\n",
      "episodes: 375\n",
      "episodes: 376\n",
      "episodes: 377\n",
      "episodes: 378\n",
      "episodes: 379\n",
      "episodes: 380\n",
      "episodes: 381\n",
      "episodes: 382\n",
      "episodes: 383\n",
      "episodes: 384\n",
      "episodes: 385\n",
      "episodes: 386\n",
      "episodes: 387\n",
      "episodes: 388\n",
      "episodes: 389\n",
      "episodes: 390\n",
      "episodes: 391\n",
      "episodes: 392\n",
      "episodes: 393\n",
      "episodes: 394\n",
      "episodes: 395\n",
      "episodes: 396\n",
      "episodes: 397\n",
      "episodes: 398\n",
      "episodes: 399\n",
      "episodes: 400\n",
      "episodes: 401\n",
      "episodes: 402\n",
      "episodes: 403\n",
      "episodes: 404\n",
      "episodes: 405\n",
      "episodes: 406\n",
      "episodes: 407\n",
      "episodes: 408\n",
      "episodes: 409\n",
      "episodes: 410\n",
      "episodes: 411\n",
      "episodes: 412\n",
      "episodes: 413\n",
      "episodes: 414\n",
      "episodes: 415\n",
      "episodes: 416\n",
      "episodes: 417\n",
      "episodes: 418\n",
      "episodes: 419\n",
      "episodes: 420\n",
      "episodes: 421\n",
      "episodes: 422\n",
      "episodes: 423\n",
      "episodes: 424\n",
      "episodes: 425\n",
      "episodes: 426\n",
      "episodes: 427\n",
      "episodes: 428\n",
      "episodes: 429\n",
      "episodes: 430\n",
      "episodes: 431\n",
      "episodes: 432\n",
      "episodes: 433\n",
      "episodes: 434\n",
      "episodes: 435\n",
      "episodes: 436\n",
      "episodes: 437\n",
      "episodes: 438\n",
      "episodes: 439\n",
      "episodes: 440\n",
      "episodes: 441\n",
      "episodes: 442\n",
      "episodes: 443\n",
      "episodes: 444\n",
      "episodes: 445\n",
      "episodes: 446\n",
      "episodes: 447\n",
      "episodes: 448\n",
      "episodes: 449\n",
      "episodes: 450\n",
      "episodes: 451\n",
      "episodes: 452\n",
      "episodes: 453\n",
      "episodes: 454\n",
      "episodes: 455\n",
      "episodes: 456\n",
      "episodes: 457\n",
      "episodes: 458\n",
      "episodes: 459\n",
      "episodes: 460\n",
      "episodes: 461\n",
      "episodes: 462\n",
      "episodes: 463\n",
      "episodes: 464\n",
      "episodes: 465\n",
      "episodes: 466\n",
      "episodes: 467\n",
      "episodes: 468\n",
      "episodes: 469\n",
      "episodes: 470\n",
      "episodes: 471\n",
      "episodes: 472\n",
      "episodes: 473\n",
      "episodes: 474\n",
      "episodes: 475\n",
      "episodes: 476\n",
      "episodes: 477\n",
      "episodes: 478\n",
      "episodes: 479\n",
      "episodes: 480\n",
      "episodes: 481\n",
      "episodes: 482\n",
      "episodes: 483\n",
      "episodes: 484\n",
      "episodes: 485\n",
      "episodes: 486\n",
      "episodes: 487\n",
      "episodes: 488\n",
      "episodes: 489\n",
      "episodes: 490\n",
      "episodes: 491\n",
      "episodes: 492\n",
      "episodes: 493\n",
      "episodes: 494\n",
      "episodes: 495\n",
      "episodes: 496\n",
      "episodes: 497\n",
      "episodes: 498\n",
      "episodes: 499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import json\n",
    "from gamelogic.game import Game\n",
    "import time\n",
    "from shutil import copyfile\n",
    "import parameters\n",
    "import os\n",
    "\n",
    "EPISODES = 500\n",
    "\n",
    "path  = os.getcwd()\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = 16\n",
    "        self.action_size = 4 # (up, down, right, left)\n",
    "        self.memory = deque(maxlen=5000000)\n",
    "        self.gamma = parameters.gamma    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = parameters.epsilon_decay\n",
    "        self.learning_rate = parameters.learning_rate\n",
    "        self.model = self._build_model()\n",
    "        self.batch_size = parameters.batch_size\n",
    "        self.is_max_value_reward = parameters.is_max_value_reward\n",
    "        self.max_value_reward_threshold = parameters.max_value_reward_threshold\n",
    "        self.max_value_reward_amount = parameters.max_value_reward_amount\n",
    "        self.output_name = parameters.output_name\n",
    "        filename = path + \"/data/\"\n",
    "        if not os.path.exists(filename):\n",
    "            os.makedirs(filename)\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='relu'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"algorithm tends to forget the previous experiences as it overwrites them with new experiences.\n",
    "        Therefore we re-train the model with previous experiences.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(game.available_actions())\n",
    "        #forward feeding\n",
    "        act_values = self.model.predict(state)\n",
    "        #sets q-values of not available actions to -100 so they are not chosen\n",
    "        if len(game.available_actions())< 4:\n",
    "          temp = game.available_actions()\n",
    "          for i in range(0, 4):\n",
    "            if i not in temp:\n",
    "              act_values[0][i] = -100\n",
    "        #returns action with highest q-value\n",
    "        return np.argmax(act_values[0])\n",
    "        #replace return with this for a random agent:\n",
    "        #return random.choice(game.available_actions())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"trains the neural net with experiences from memory (minibatches)\"\"\"\n",
    "        #samples mimibatch from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        #for each memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #if its final state set target to the reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                #set target according to formula\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            #gets all 4 predictions from current state\n",
    "            target_f = self.model.predict(state)\n",
    "            #takes the one action which was selected in batch\n",
    "            target_f[0][action] = target\n",
    "            #trains the model\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    game = Game()\n",
    "    agent = DQNAgent()\n",
    "    # agent.load(\"./save/file\")\n",
    "    done = False\n",
    "    batch_size = agent.batch_size\n",
    "    debug = False\n",
    "    save_maxvalues = True\n",
    "    output_list = []\n",
    "\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        game.new_game()\n",
    "        state = game.state()\n",
    "        state = np.reshape(state, [1, agent.state_size])\n",
    "        while not game.game_over():\n",
    "            action = agent.act(state)\n",
    "            reward = (game.do_action(action))**2\n",
    "            if(agent.is_max_value_reward):\n",
    "                reward = 0\n",
    "                temp = game.state()\n",
    "                temp_reshaped = np.reshape(temp, [1, agent.state_size])\n",
    "                temp_max_value = np.amax(temp_reshaped[0])\n",
    "                if temp_max_value > agent.max_value_reward_threshold:\n",
    "                    reward = agent.max_value_reward_amount\n",
    "            next_state = game.state()\n",
    "            actions_available = game.available_actions()\n",
    "            if len(actions_available) == 0:\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "            next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if (debug): print(\"no action available\")\n",
    "                states = game.state()\n",
    "                states = np.reshape(state, [1, agent.state_size])\n",
    "                max_value = np.amax(states[0])\n",
    "                output_list.append([e, np.asscalar(max_value), np.asscalar(game.score()), agent.epsilon])\n",
    "                if(debug):print(\"max_value: \" + str(max_value))\n",
    "                break\n",
    "        print(\"episodes: \" + str(e))\n",
    "\n",
    "        #save copy of configuration and the episode_maxvalue_data\n",
    "        if save_maxvalues:\n",
    "            if e == 100:\n",
    "                output_list.insert(0, \"gamma: \"+str(parameters.gamma)+\" | epsilon decay: \"+str(parameters.epsilon_decay)+\" | learning rate: \"+str(parameters.learning_rate)+\"\\n batch size: \"+str(parameters.batch_size)+\" | reward = maxVal: \"+str(parameters.is_max_value_reward)+\" | reward amount: \"+str(parameters.max_value_reward_amount)+\" | reward threshold: \"+str(parameters.max_value_reward_threshold))\n",
    "            if e % 100 == 0:\n",
    "                with open(path + \"/data/\"+agent.output_name+\"output.txt\", \"w\") as outfile:\n",
    "                    json.dump(output_list, outfile)\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        if e % 1000 == 0:\n",
    "            timenow = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            savepath = path + \"/data/checkpoint\"\n",
    "            agent.save(savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"ptl\"> 2.5.8 Plotting </a> \n",
    "\n",
    "As Plotting helps a lot to get the hyperparameters right and see the performance of the agent, a plotting function was developed to easily turn the training-data to a plot.\n",
    "To plot the values, just execute the following cell.\n",
    "By default, the average score, the average max value and the decaying epsilon are being plotted taking an average from a sample of the size set as the parameter stepsize, for example the first point is the average of the first 100 scores if the stepsize is set to 100.<br>\n",
    "If plot_max_instead_of_avg is set to TRUE, then instead of the average, the maximum values of every sample is plotted.<br>\n",
    "The inputname sets the name of the json file for the input data and therefore has to match the parameter set in parameters.py at the time of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#plotting info from https://matplotlib.org/gallery/ticks_and_spines/multiple_yaxis_with_spines.html\n",
    "\n",
    "\n",
    "#####SET PARAMETERS HERE######\n",
    "stepsize = 100\n",
    "plot_max_instead_of_avg = False\n",
    "inputname = \"test\"\n",
    "debug = False\n",
    "#####SET PARAMETERS HERE######\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "def plot():\n",
    "    with open(path + '/data/'+inputname+'output.txt', \"r\") as infile:\n",
    "      inputlist = json.load(infile)\n",
    "      if (debug): print(\"path is: \" + str(path))\n",
    "      if (debug): print(\"list is: \" + str(inputlist))\n",
    "      #extract first item from list which is the configuration info\n",
    "\n",
    "      plottitle = inputlist[0]\n",
    "      inputlist.pop(0)\n",
    "      calculate_data(inputlist, plottitle)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_data(inputlist, plottitle):\n",
    "  number_of_steps = int(len(inputlist) / stepsize)\n",
    "  if(debug):print(\"nr of steps is: \" + str(number_of_steps))\n",
    "\n",
    "  # EPISODES\n",
    "  episodes_list = []\n",
    "  for i in range(1, number_of_steps + 1):\n",
    "    episodes_list.append(i * stepsize);\n",
    "  if (debug):print(\"episode list: \" + str(episodes_list))\n",
    "\n",
    "  # Average Score\n",
    "  avg_score_list = []\n",
    "  for i in range(0, number_of_steps):\n",
    "    sum = 0\n",
    "    for j in range(0, stepsize):\n",
    "      sum = sum + inputlist[stepsize * i + j][2]\n",
    "    result = sum / (stepsize)\n",
    "    avg_score_list.append(result)\n",
    "  if (debug):print(\"avg_score_list: \" + str(avg_score_list))\n",
    "\n",
    "  # Average MaxValue\n",
    "  avg_max_value_list = []\n",
    "  for i in range(0, number_of_steps):\n",
    "    sum = 0\n",
    "    for j in range(0, stepsize):\n",
    "      sum = sum + inputlist[stepsize * i + j][1]\n",
    "    result = sum / float(stepsize)\n",
    "    avg_max_value_list.append(result)\n",
    "  if (debug):print(\"avg_max_value_list: \" + str(avg_max_value_list))\n",
    "\n",
    "  # Epsilon\n",
    "  epsilon_list = []\n",
    "  for i in range(0, number_of_steps):\n",
    "    result = inputlist[stepsize * i][3]\n",
    "    epsilon_list.append(result)\n",
    "  if (debug):print(\"epsilon_list: \" + str(epsilon_list))\n",
    "\n",
    "  # MaxScore\n",
    "  max_score_list = []\n",
    "  # make now list consisting only of scores\n",
    "  templist = [i[2] for i in inputlist]\n",
    "  for i in range(0, number_of_steps):\n",
    "    result = max(templist[i * stepsize:((i + 1) * stepsize)])\n",
    "    max_score_list.append(result)\n",
    "  if (debug):print(\"max_score: \" + str(max_score_list))\n",
    "\n",
    "  # MaxValue\n",
    "  max_value_list = []\n",
    "  # make now list consisting only of scores\n",
    "  templist_two = [i[1] for i in inputlist]\n",
    "  for i in range(0, number_of_steps):\n",
    "    result = max(templist_two[i * stepsize:((i + 1) * stepsize)])\n",
    "    max_value_list.append(result)\n",
    "  if (debug):print(\"max_value_list: \" + str(max_value_list))\n",
    "\n",
    "\n",
    "  if(plot_max_instead_of_avg):\n",
    "      plot_data(plottitle, max_x=len(inputlist)-(len(inputlist)%stepsize), para_episodes_list=episodes_list, para_value_list=max_value_list, para_score_list=max_score_list, para_epsilon_list=epsilon_list)\n",
    "  else:\n",
    "      plot_data(plottitle, max_x=len(inputlist)-(len(inputlist)%stepsize), para_episodes_list=episodes_list, para_value_list=avg_max_value_list, para_score_list=avg_score_list, para_epsilon_list=epsilon_list)\n",
    "\n",
    "\n",
    "def plot_data(plottitle, max_x, para_episodes_list, para_value_list, para_score_list, para_epsilon_list):\n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "\n",
    "\n",
    "    fig, host = plt.subplots()\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "    par1 = host.twinx()\n",
    "    par2 = host.twinx()\n",
    "    par2.spines[\"right\"].set_position((\"axes\", 1.2))\n",
    "    make_patch_spines_invisible(par2)\n",
    "    par2.spines[\"right\"].set_visible(True)\n",
    "\n",
    "    #Score\n",
    "    p1, = host.plot(para_episodes_list, para_score_list, \"b-\", label=\"Score\")\n",
    "    #MaxValue\n",
    "    p2, = par1.plot(para_episodes_list, para_value_list, \"r-\", label=\"MaxValue\")\n",
    "    #Epsilon\n",
    "    p3, = par2.plot(para_episodes_list, para_epsilon_list, \"g-\", label=\"Epsilon\")\n",
    "\n",
    "    #Episodes\n",
    "    host.set_xlim(stepsize, max_x)\n",
    "    #Score\n",
    "    host.set_ylim(0, 2500)\n",
    "    if (plot_max_instead_of_avg): host.set_ylim(0, 9000)\n",
    "    #MaxValue\n",
    "    par1.set_ylim(6, 8)\n",
    "    if (plot_max_instead_of_avg): par1.set_ylim(6, 11)\n",
    "    #Epsilon\n",
    "    par2.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "    host.set_xlabel(\"Episodes\")\n",
    "    host.set_ylabel(\"Score\")\n",
    "    par1.set_ylabel(\"Max_Value\")\n",
    "    par2.set_ylabel(\"Epsilon\")\n",
    "    host.yaxis.label.set_color(p1.get_color())\n",
    "    par1.yaxis.label.set_color(p2.get_color())\n",
    "    par2.yaxis.label.set_color(p3.get_color())\n",
    "    tkw = dict(size=4, width=1.5)\n",
    "    host.tick_params(axis='y', colors=p1.get_color(), **tkw)\n",
    "    par1.tick_params(axis='y', colors=p2.get_color(), **tkw)\n",
    "    par2.tick_params(axis='y', colors=p3.get_color(), **tkw)\n",
    "    host.tick_params(axis='x', **tkw)\n",
    "    lines = [p1, p2, p3]\n",
    "    host.legend(lines, [l.get_label() for l in lines])\n",
    "    plt.title(plottitle, fontsize=10)\n",
    "    additionalname = \"_avg_\"\n",
    "    if (plot_max_instead_of_avg): additionalname = \"_max_\"\n",
    "\n",
    "    figurepath = path + \"/graphs/\"\n",
    "    if not os.path.exists(figurepath):\n",
    "      os.makedirs(figurepath)\n",
    "    fig.savefig(figurepath + str(inputname)+additionalname+\"graph.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"ree\"> 2.5.9 Results </a>\n",
    "Below is the comparison of three different configurations which we found to be the most successful configurations and a random moving agent.<br>\n",
    "On the left side, each point on the plot reflects an average over 1000 games each. On the right side, each point reflects the maximum value in terms of score or the maximum tile over 1000 games each.\n",
    "(a) used a score based reward and (b) and (c) used maximum value-based rewards. <br>\n",
    "The difference between the latter two is that we set (c) up for a long training time and therefore set a higher threshold for the tile-value which it has to reach to get a reward (10 instead of 9). Note: a 9 would be a tile of value 2 to the power of 9 = 512 and the threshold has to be exceeded to get the reward, meaning if reward_threshold is 8, the agent has to reach a 10^9 to get the reward. <br>\n",
    "Overall we see a significant improvement in score and max_value compared to a random agent.<br> Our agent reached an average maxVal of 7.25 and an average score of 2200 after 60’000 episodes compared to an average maxVal of 6.58 and average score of 1050 of a random agent.<br>\n",
    "The comparison of lefthand side graphs (b) and (c) shows that there was no improvement in the averages if a higher threshold for the maximum value is chosen (2^10 instead of 2^9) and at the same time the number of episodes is drastically increased.<br>\n",
    "However on the right side we see that with more episodes, higher scores and max_values are reached in single games. <br>\n",
    "We think this shows that it is too unlikely to reach the defined reward threshold of 2^10 by random play and given the randomness in the game even harder to learn from those few occurences. <br>\n",
    "The highest tile we could reach was 2^10 which is 1024. Considering the exponential difficulty of the game, reaching 2048 would need a lot more training and optimization. <br>\n",
    "While looking at single games played from a trained agent, there is a clear trend that it uses left and up much more often then right or down. This would hint that the agent has learned a similar strategy to the \"corner-strategy\" mentioned above, because this strategy tends to put high values in the upper right corner.\n",
    "\n",
    "<img src=\"notebook_img/plot_comparison.png\" alt=\"Plots\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"pgg\"> 2.5.10 Playing the game </a>\n",
    "During training, Keras saves a checkpoint of the model every 1000 episodes. When running the following cell, One game is played using the latest checkpoint, displaying the game grid after every move. <br>\n",
    "There is also the possiblily to load the agent (a) and (b) from the plots above to skip learning and use a trained agent right away. For this use one of agents file agent_a and agent_b included in the data folder and rename it to checkpoint, so it will automatically be used by play_single_game()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |     |     |     |\n",
      "-------------------------\n",
      "|    2|     |     |     |\n",
      "-------------------------\n",
      "|     |     |     |    2|\n",
      "-------------------------\n",
      "|     |     |     |    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |     |     |     |\n",
      "-------------------------\n",
      "|     |     |    4|    2|\n",
      "-------------------------\n",
      "|     |     |     |    2|\n",
      "-------------------------\n",
      "|     |     |     |    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |     |     |     |\n",
      "-------------------------\n",
      "|    4|    2|     |     |\n",
      "-------------------------\n",
      "|    2|     |     |     |\n",
      "-------------------------\n",
      "|    2|     |    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|     |     |     |\n",
      "-------------------------\n",
      "|     |     |    4|    2|\n",
      "-------------------------\n",
      "|     |     |     |    2|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|     |     |    2|\n",
      "-------------------------\n",
      "|     |     |    4|    2|\n",
      "-------------------------\n",
      "|     |     |     |    2|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "|     |     |    4|    2|\n",
      "-------------------------\n",
      "|     |     |     |    2|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|     |     |\n",
      "-------------------------\n",
      "|    4|    2|     |     |\n",
      "-------------------------\n",
      "|    2|     |     |    2|\n",
      "-------------------------\n",
      "|    4|     |     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "|    2|     |    4|    2|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "|    2|     |     |    4|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "|    2|     |    2|    4|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |     |    4|    4|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|     |    2|    4|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |     |     |    8|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|     |     |    4|    4|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |    4|     |    8|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|     |     |     |    8|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |     |    4|    8|\n",
      "-------------------------\n",
      "|    2|     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |     |    2|    8|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |     |    4|    8|\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|    2|     |\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|    4|    8|     |     |\n",
      "-------------------------\n",
      "|    2|    4|     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |     |    4|    8|\n",
      "-------------------------\n",
      "|     |    2|    2|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |     |    4|    8|\n",
      "-------------------------\n",
      "|    2|     |    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |     |    4|    8|\n",
      "-------------------------\n",
      "|     |    2|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|     |    2|    4|    8|\n",
      "-------------------------\n",
      "|     |     |    4|    8|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|    2|     |\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "|    4|    8|     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "|     |    2|    4|    8|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|    2|     |\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    2|    8|    2|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "|    4|    2|    4|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 1296\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|    4|    2|    4|    4|\n",
      "-------------------------\n",
      "|    4|    8|   16|     |\n",
      "-------------------------\n",
      "|     |    2|     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|     |    4|    2|    8|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|     |     |    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|     |    4|    2|    8|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|     |    2|     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|     |    4|    2|    8|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|     |    2|    2|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    2|    8|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|     |     |    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    2|    8|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|     |     |    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    2|    8|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|     |    2|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    2|    8|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|     |     |    4|    8|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    2|    8|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    4|    8|     |    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    2|    8|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 1600\n",
      "-------------------------\n",
      "|    4|    8|    8|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|     |     |   16|   16|\n",
      "-------------------------\n",
      "|    2|     |     |    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 2704\n",
      "-------------------------\n",
      "|     |    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|     |     |    2|   32|\n",
      "-------------------------\n",
      "|     |     |     |    4|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    4|   16|    4|     |\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|    2|   32|    2|     |\n",
      "-------------------------\n",
      "|    4|     |     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|     |    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|     |    2|   32|    2|\n",
      "-------------------------\n",
      "|    2|     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|     |    2|   32|    2|\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|    2|   32|    2|    4|\n",
      "-------------------------\n",
      "|    2|    4|     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|    2|   32|    2|    4|\n",
      "-------------------------\n",
      "|    2|     |    2|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|    2|   32|    2|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|    2|   32|    2|    4|\n",
      "-------------------------\n",
      "|     |    2|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|    2|   32|    2|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    8|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    2|    8|\n",
      "-------------------------\n",
      "|    2|   32|    2|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|     |    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|    8|    4|\n",
      "-------------------------\n",
      "|    2|    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|    8|    4|\n",
      "-------------------------\n",
      "|    4|    2|    8|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 576\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    8|    8|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|     |    2|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|     |   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|    2|     |    2|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|     |   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|    2|     |    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|     |   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|     |    2|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|     |   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|     |    4|    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|     |     |    8|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|     |     |    2|   16|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|    2|   16|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|    2|    2|   16|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   32|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 1296\n",
      "-------------------------\n",
      "|    4|    4|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|    2|   32|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|     |    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 5184\n",
      "-------------------------\n",
      "|     |    8|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|    2|    2|   64|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|     |    8|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|     |    4|   64|    4|\n",
      "-------------------------\n",
      "|    2|    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|     |    4|   64|    4|\n",
      "-------------------------\n",
      "|     |    4|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|     |    4|   64|    4|\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   64|    4|    2|\n",
      "-------------------------\n",
      "|    2|    8|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   64|    4|    2|\n",
      "-------------------------\n",
      "|    2|    2|    8|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|    4|    8|\n",
      "-------------------------\n",
      "|    4|   64|    4|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    8|   16|    8|    8|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|     |    4|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|     |    8|   16|   16|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 1024\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|     |    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    2|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|    2|    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 144\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    4|     |    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|     |    8|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|     |    2|    2|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    2|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|   64|    8|    4|\n",
      "-------------------------\n",
      "|    4|    2|    8|   16|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    4|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    4|   64|    8|    4|\n",
      "-------------------------\n",
      "|     |    2|    2|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    4|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    4|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    4|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    4|   64|    8|    4|\n",
      "-------------------------\n",
      "|    2|    4|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    4|    8|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    4|   64|    8|    4|\n",
      "-------------------------\n",
      "|    4|    2|    8|   16|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 3136\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    8|   64|     |    4|\n",
      "-------------------------\n",
      "|     |    2|    2|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    2|    8|   64|    4|\n",
      "-------------------------\n",
      "|     |     |    4|   16|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    2|    8|   64|    4|\n",
      "-------------------------\n",
      "|    4|   16|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    2|    8|   64|    4|\n",
      "-------------------------\n",
      "|    4|    4|   16|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|   32|\n",
      "-------------------------\n",
      "|    2|    8|   64|    4|\n",
      "-------------------------\n",
      "|    2|    8|   16|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    4|    4|   16|   32|\n",
      "-------------------------\n",
      "|    2|   16|   64|    4|\n",
      "-------------------------\n",
      "|     |    2|   16|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|   16|   64|    4|\n",
      "-------------------------\n",
      "|     |    2|   16|    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|   16|   64|    4|\n",
      "-------------------------\n",
      "|    2|   16|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|   16|   64|    4|\n",
      "-------------------------\n",
      "|    2|    2|   16|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|    8|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|   16|   64|    4|\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 784\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|   16|   16|   32|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|     |    2|   16|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 1024\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|   32|   32|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|     |     |    2|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|   64|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|    2|     |    2|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    2|    4|   64|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|     |     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    4|    4|   64|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|     |     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|     |    2|    8|   64|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|    2|     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    2|    8|   64|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|     |    2|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|     |    4|    8|   64|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|    4|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    4|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|    8|   64|    2|\n",
      "-------------------------\n",
      "|    2|    4|   64|    8|\n",
      "-------------------------\n",
      "|    4|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 18496\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|    4|    4|    4|    8|\n",
      "-------------------------\n",
      "|     |    2|    2|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 144\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|     |    4|    8|    8|\n",
      "-------------------------\n",
      "|    2|     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|     |    2|    4|   16|\n",
      "-------------------------\n",
      "|     |    2|    4|   16|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    2|\n",
      "-------------------------\n",
      "|    2|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    2|\n",
      "-------------------------\n",
      "|    2|    2|    8|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    2|    8|  128|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|    8|  128|    4|\n",
      "-------------------------\n",
      "|    2|    8|   16|   16|\n",
      "-------------------------\n",
      "|     |     |    8|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 1024\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|    8|  128|    4|\n",
      "-------------------------\n",
      "|     |    2|    8|   32|\n",
      "-------------------------\n",
      "|    2|     |    8|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|    8|  128|    4|\n",
      "-------------------------\n",
      "|    4|    2|    8|   32|\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|    8|  128|    4|\n",
      "-------------------------\n",
      "|    4|    2|    8|   32|\n",
      "-------------------------\n",
      "|    2|    8|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|    8|  128|    4|\n",
      "-------------------------\n",
      "|    4|    2|    8|   32|\n",
      "-------------------------\n",
      "|    2|    2|    8|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   32|    4|\n",
      "-------------------------\n",
      "|    4|    8|  128|    4|\n",
      "-------------------------\n",
      "|    4|    2|    8|   32|\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 1024\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|    8|    8|  128|   32|\n",
      "-------------------------\n",
      "|    2|    2|   16|    4|\n",
      "-------------------------\n",
      "|     |    4|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|     |   16|  128|   32|\n",
      "-------------------------\n",
      "|     |    4|   16|    4|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|    4|     |\n",
      "-------------------------\n",
      "|    2|    4|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|     |    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|     |    4|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|     |    2|    8|    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|    8|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|    2|    8|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   32|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|   16|    4|\n",
      "-------------------------\n",
      "|    2|    4|    8|    4|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 7056\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|    8|    8|\n",
      "-------------------------\n",
      "|    2|     |     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|     |     |    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|     |    4|    8|   16|\n",
      "-------------------------\n",
      "|     |     |    2|    4|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|     |\n",
      "-------------------------\n",
      "|    2|    4|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|     |    2|    4|    2|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    2|    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 144\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|     |    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    2|    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    2|    8|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    2|    8|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 3600\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|     |    2|     |     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|     |     |    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|     |     |    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|     |    2|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|     |    2|    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|     |    2|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|     |    2|    4|    8|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    2|\n",
      "-------------------------\n",
      "|    4|    8|   16|   32|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 1024\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|    8|    8|   32|\n",
      "-------------------------\n",
      "|    2|    4|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|     |    4|   16|   32|\n",
      "-------------------------\n",
      "|    2|    2|    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 144\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|     |    4|   16|   32|\n",
      "-------------------------\n",
      "|     |    2|    4|    8|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    8|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    8|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   32|    2|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 4624\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   64|    4|\n",
      "-------------------------\n",
      "|    4|   16|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    2|     |\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   64|    4|\n",
      "-------------------------\n",
      "|    4|   16|    8|   16|\n",
      "-------------------------\n",
      "|    2|    2|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   64|    4|\n",
      "-------------------------\n",
      "|    4|   16|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    4|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   64|    4|\n",
      "-------------------------\n",
      "|    4|   16|    8|   16|\n",
      "-------------------------\n",
      "|    2|    2|    8|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|   64|    8|\n",
      "-------------------------\n",
      "|   16|  128|   64|    4|\n",
      "-------------------------\n",
      "|    4|   16|    8|   16|\n",
      "-------------------------\n",
      "|    2|    4|    8|    2|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 20736\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|     |   16|\n",
      "-------------------------\n",
      "|    2|    4|    2|    2|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 1296\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|     |    4|   32|\n",
      "-------------------------\n",
      "|     |    2|    4|    4|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|     |     |    8|   32|\n",
      "-------------------------\n",
      "|    4|     |    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    2|     |    8|   32|\n",
      "-------------------------\n",
      "|     |    4|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|     |    2|    8|   32|\n",
      "-------------------------\n",
      "|    4|    4|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    2|    2|    8|   32|\n",
      "-------------------------\n",
      "|     |    8|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|     |    4|    8|   32|\n",
      "-------------------------\n",
      "|    2|    8|    2|    8|\n",
      "-------------------------\n",
      "Action: left\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|    8|   32|    2|\n",
      "-------------------------\n",
      "|    2|    8|    2|    8|\n",
      "-------------------------\n",
      "Action: up\n",
      "Reward: 256\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    2|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|    4|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    8|    8|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 400\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|     |    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 0\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|    4|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 64\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    2|    8|   16|\n",
      "-------------------------\n",
      "Action: right\n",
      "Reward: 16\n",
      "-------------------------\n",
      "|    8|   16|  128|    8|\n",
      "-------------------------\n",
      "|   16|  128|   16|    4|\n",
      "-------------------------\n",
      "|    4|   16|   32|    2|\n",
      "-------------------------\n",
      "|    2|    4|    8|   16|\n",
      "-------------------------\n",
      "Score: 1904\n",
      "Max Value: 128\n",
      "Game over.\n"
     ]
    }
   ],
   "source": [
    " from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from keras.models import load_model\n",
    "from gamelogic.game import Game, ACTION_NAMES\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "\n",
    "def play_single_game():\n",
    "    \"\"\"Play a single game using the latest model snapshot\"\"\"\n",
    "    game = Game()\n",
    "    state_size = 16\n",
    "    debug = True\n",
    "\n",
    "\n",
    "\n",
    "    model = load_model(path + \"/data/checkpoint\")\n",
    "  \n",
    "    game.new_game()\n",
    "    state = game.state()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    while not game.game_over():\n",
    "        # get action from highest q-value\n",
    "        act_values = model.predict(state)\n",
    "        if len(game.available_actions())< 4:\n",
    "          temp = game.available_actions()\n",
    "          for i in range(0, 4):\n",
    "            if i not in temp:\n",
    "              act_values[0][i] = -100\n",
    "        #returns action with highest q-value\n",
    "        action = np.argmax(act_values[0])\n",
    "        \n",
    "        reward = (game.do_action(action))**2\n",
    "        next_state = game.state()\n",
    "        actions_available = game.available_actions()\n",
    "        if len(actions_available) == 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        state = next_state\n",
    "        print(\"Action:\", ACTION_NAMES[action])\n",
    "        print(\"Reward:\", reward)\n",
    "        game.print_state()\n",
    "\n",
    "        if done:\n",
    "            states = game.state()\n",
    "            states = np.reshape(state, [1, state_size])\n",
    "            max_value = np.amax(states[0])\n",
    "            print(\"Score:\", game.score())\n",
    "            print(\"Max Value: \" + str(2**max_value))\n",
    "            print(\"Game over.\")\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \"\"\"Main function.\"\"\"\n",
    "  play_single_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"Conclusion\"> 3. Conclusion </a>\n",
    "\n",
    "When we chose the game, we knew that it would be a hard one to play. Even for a human, there is no clear and obvious strategy in this game.\n",
    "As expected the scores were not extremely high. Training speed was slower then expected (3.5min/1000-game-episode), which was also caused by the rather slow hardware we were using.\n",
    "We also found that using a gpu did not increase the training speed, probably because of the low specs of the graphics card we used (gtx760 2GB) and the shallow network.\n",
    "This made it difficult to test the many possible combinations of hyperparameters. Still our agent is consistently beating a random player and is reaching a 1028 tile in some games, which is already rather hard to do for an unexperienced player.\n",
    "Furthermore it seems like the \"corner strategy\" is partly learned by the agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='Contribution'>4. Contribution</a>\n",
    "\n",
    " \n",
    "\n",
    "This was a fun project in which we explore the idea of Reinforcement Learning and building an application like a game of 2048 using a module deep q learning. \n",
    "       \n",
    "- Code by self : 35%\n",
    "- Code from external Sources : 65%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"Citation\"> 5. Citations </a>\n",
    "\n",
    "- https://github.com/SergioIommi/DQN-2048\n",
    "- https://github.com/dsgiitr/rl_2048\n",
    "- https://github.com/berkay-dincer/2048-ai-mcts/tree/master/src\n",
    "- https://github.com/Anaconda-Platform/nb_conda_kernels\n",
    "- https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d4\n",
    "- http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"License\"> 6. License </a>\n",
    "Copyright (c) 2020 Manali Sharma, Rushabh Nisher\n",
    "\n",
    " \n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    " \n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    " \n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python360-virtualenv",
   "language": "python",
   "name": "python360-virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
